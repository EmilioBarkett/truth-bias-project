
@misc{chen_reasoning_2025,
	title = {Reasoning {Models} {Don}’t {Always} {Say} {What} {They} {Think}},
	url = {https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf},
	author = {Chen, Yanda and Benton, Joe and Radhakrishnan, Ansh and Uesato, Jonathan and Denison, Carson and Schulman, John and Somani, Arushi and Hase, Peter and Wagner, Misha and Roger, Fabien and Mikulik, Vlad and Bowman, Sam and Leike, Jan and Kaplan, Jared and Perez, Ethan},
	month = apr,
	year = {2025},
}

@misc{greenblatt_alignment_2024,
	type = {{CS}: {AI}},
	title = {Alignment {Faking} in {Large} {Language} {Models}},
	doi = {https://doi.org/10.48550/arXiv.2412.14093},
	publisher = {arXiv},
	author = {Greenblatt, Ryan and Denison, Carson and Wright, Benjamin and Roger, Fabien and MacDiarmid, Monte and Marks, Sam and Treutlein, Johannes and Belonax, Tim and Chen, Jack and Duvenaud, David and Khan, Akbir and Michael, Julian and Mindermann, Sören and Perez, Ethan and Petrini, Linda and Uesato, Jonathan and Kaplan, Jared and Schlegeris, Buck and Bowman, Samuel R. and Hubinger, Evan},
	month = dec,
	year = {2024},
}

@article{martinez_human_2022,
	title = {Human {Cognitive} {Biases} {Present} in {Artificial} {Intelligence}},
	volume = {67},
	url = {https://www.eusko-ikaskuntza.eus/en/riev/human-cognitive-biases-present-in-artificial-intelligence/rart-24782/},
	number = {2},
	journal = {REVISTA INTERNACIONAL DE LOS ESTUDIOS VASCOS. RIEV},
	author = {Martínez, Naroa and Agudo, Ujué and Matute, Helena},
	month = nov,
	year = {2022},
}

@article{chen_ethics_2023,
	title = {Ethics and discrimination in artificial intelligence-enabled recruitment practices},
	volume = {10},
	journal = {Humanities and Social Sciences Communications},
	author = {Chen, Zhisheng},
	month = sep,
	year = {2023},
}

@article{ferrara_fairness_2023,
	title = {Fairness and {Bias} in {Artificial} {Intelligence}: {A} {Brief} {Survey} of {Sources}, {Impacts}, and {Mitigation} {Strategies}},
	volume = {6},
	doi = {https://doi.org/10.3390/sci6010003},
	number = {1},
	journal = {Sci},
	author = {Ferrara, Emilio},
	month = dec,
	year = {2023},
}

@misc{saeedi_heuristics_2025,
	title = {Heuristics and {Biases} in {AI} {Decision}-{Making}: {Implications} for {Responsible} {AGI}},
	publisher = {arXiv},
	author = {Saeedi, Payam and Goodarzi, Mahsa and Canbaz, Abdullah},
	month = apr,
	year = {2025},
}

@inproceedings{wang_primacy_2023,
	title = {Primacy {Effect} of {ChatGPT}},
	doi = {https://doi.org/10.48550/arXiv.2310.13206},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Wang, Yiwei and Cai, Yujun and Chen, Muhao and Liang, Yuxuan and Hooi, Bryan},
	month = jan,
	year = {2023},
}

@article{mesoudi_multiple_2008,
	title = {The multiple roles of cultural transmission experiments in understanding human cultural evolution},
	volume = {363},
	doi = {https://doi.org/10.1098/rstb.2008.0129},
	number = {1509},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Mesoudi, Alex and Whiten, Andrew},
	month = nov,
	year = {2008},
}

@article{acerbi_large_2023,
	title = {Large language models show human-like content biases in transmission chain experiments},
	volume = {120},
	number = {44},
	journal = {PNAS},
	author = {Acerbi, Alberto and Stubbersfield, Joseph M.},
	month = oct,
	year = {2023},
}

@article{pantazi_power_2018,
	title = {The {Power} of the {Truth} {Bias}: {False} {Information} {Affects} {Memory} and {Judgment} {Even} in the {Absence} of {Distraction}},
	volume = {36},
	number = {2},
	journal = {Social Cognition},
	author = {Pantazi, Myrto and Kissine, Mikhail and Klein, Oliver},
	month = apr,
	year = {2018},
}

@article{reinhardt_relationship_2025,
	title = {On the relationship between honesty-humility and truth-bias},
	volume = {116},
	journal = {Journal of Research in Personality},
	author = {Reinhardt, Nina and Schindler, Simon},
	month = jun,
	year = {2025},
}

@article{levine_truth-default_2014,
	title = {Truth-{Default} {Theory} ({TDT}): {A} {Theory} of {Human} {Deception} and {Deception} {Detection}},
	volume = {33},
	doi = {https://doi.org/10.1177/0261927X1453591},
	number = {4},
	journal = {Journal of Language and Social Psychology},
	author = {Levine, Timothy R.},
	month = may,
	year = {2014},
}

@article{mccornack_deception_1986,
	title = {Deception {Detection} and
Relationship {Development}:
The {Other} {Side} of {Trust}},
	volume = {9},
	doi = {https://doi.org/10.1080/23808985.1986.11678616},
	number = {1},
	journal = {Annals of the International Communication Association},
	author = {McCornack, Steven A. and Parks},
	month = jan,
	year = {1986},
	pages = {377--389},
}

@article{banerjee_calling_2021,
	title = {Calling out fake online reviews through robust epistemic belief},
	volume = {58},
	doi = {https://doi.org/10.1016/j.im.2021.103445},
	number = {3},
	journal = {Information \& Management},
	author = {Banerjee, Snehasish and Chua, Alton YK},
	month = apr,
	year = {2021},
}

@inproceedings{levitan_linguistic_2018,
	address = {New Orleans, Louisiana},
	title = {Linguistic {Cues} to {Deception} and {Perceived} {Deception} in {Interview} {Dialogues}},
	url = {https://aclanthology.org/N18-1176/},
	doi = {10.18653/v1/N18-1176},
	abstract = {We explore deception detection in interview dialogues. We analyze a set of linguistic features in both truthful and deceptive responses to interview questions. We also study the perception of deception, identifying characteristics of statements that are perceived as truthful or deceptive by interviewers. Our analysis show significant differences between truthful and deceptive question responses, as well as variations in deception patterns across gender and native language. This analysis motivated our selection of features for machine learning experiments aimed at classifying globally deceptive speech. Our best classification performance is 72.74\% F1-Score (about 17\% better than human performance), which is achieved using a combination of linguistic features and individual traits.},
	urldate = {2025-04-11},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Levitan, Sarah Ita and Maredia, Angel and Hirschberg, Julia},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {1941--1950},
}

@article{hancock_lies_nodate,
	title = {Lies in {Conversation}: {An} {Examination} of {Deception} {Using} {Automated} {Linguistic} {Analysis}},
	abstract = {The present study investigated changes in both the sender’s and the receiver’s linguistic style across truthful and deceptive dyadic communication. A computer-based analysis of 242 transcripts revealed that senders used more words overall, increased references to others, and used more sense-based descriptions (e.g., seeing, touching) when lying as compared to telling the truth. Receivers naïve to the deception manipulation produced more words and sense terms, and asked more questions with shorter sentences when they were being lied to than when they were being told the truth. These findings are discussed in terms of their implications for linguistic style matching.},
	language = {en},
	author = {Hancock, Jeffrey T and Curry, Lauren E and Goorha, Saurabh and Woodworth, Michael T},
}

@article{depaulo_cues_2003,
	title = {Cues to deception.},
	volume = {129},
	issn = {1939-1455, 0033-2909},
	url = {https://doi.apa.org/doi/10.1037/0033-2909.129.1.74},
	doi = {10.1037/0033-2909.129.1.74},
	language = {en},
	number = {1},
	urldate = {2025-04-11},
	journal = {Psychological Bulletin},
	author = {DePaulo, Bella M. and Lindsay, James J. and Malone, Brian E. and Muhlenbruck, Laura and Charlton, Kelly and Cooper, Harris},
	year = {2003},
	pages = {74--118},
}

@article{newman_lying_2003,
	title = {Lying {Words}: {Predicting} {Deception} from {Linguistic} {Styles}},
	volume = {29},
	copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {0146-1672, 1552-7433},
	shorttitle = {Lying {Words}},
	url = {https://journals.sagepub.com/doi/10.1177/0146167203029005010},
	doi = {10.1177/0146167203029005010},
	abstract = {Telling lies often requires creating a story about an experience or attitude that does not exist. As a result, false stories may be qualitatively different from true stories. The current project investigated the features of linguistic style that distinguish between true and false stories. In an analysis of five independent samples, a computer-based text analysis program correctly classified liars and truth-tellers at a rate of 67\% when the topic was constant and a rate of 61\% overall. Compared to truth-tellers, liars showed lower cognitive complexity, used fewer self-references and other-references, and used more negative emotion words.},
	language = {en},
	number = {5},
	urldate = {2025-04-11},
	journal = {Personality and Social Psychology Bulletin},
	author = {Newman, Matthew L. and Pennebaker, James W. and Berry, Diane S. and Richards, Jane M.},
	month = may,
	year = {2003},
	pages = {665--675},
}

@article{chen_reasoning_nodate,
	title = {Reasoning {Models} {Don}’t {Always} {Say} {What} {They} {Think}},
	abstract = {Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model’s CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models’ actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1\% of examples where they use the hint, but the reveal rate is often below 20\%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.},
	language = {en},
	author = {Chen, Yanda and Benton, Joe and Radhakrishnan, Ansh and Uesato, Jonathan and Denison, Carson and Schulman, John and Somani, Arushi and Hase, Peter and Wagner, Misha and Roger, Fabien and Mikulik, Vlad and Bowman, Sam and Leike, Jan and Kaplan, Jared and Perez, Ethan},
}

@misc{anthropic_claude_2024,
	title = {The {Claude} 3 {Model} {Family}: {Opus}, {Sonnet}, {Haiku}},
	url = {https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf},
	author = {Anthropic},
	month = oct,
	year = {2024},
}

@article{johnson_mixing_1988,
	title = {Mixing {Humans} and {Nonhumans} {Together}: {The} {Sociology} of a {Door}-{Closer}},
	volume = {35},
	issn = {0037-7791},
	shorttitle = {Mixing {Humans} and {Nonhumans} {Together}},
	url = {https://www.jstor.org/stable/800624},
	doi = {10.2307/800624},
	abstract = {Is sociology the study of social questions, or is it the study of associations? In this paper the author takes the second position and extends the study of our associations to nonhumans. To make the argument clearer, the author chooses one very humble nonhuman, a door-closer, and analyzes how this "purely" technical artifact is a highly moral, highly social actor that deserves careful consideration. Then the author proposes a vocabulary to follow human and nonhuman relations without stopping at artificial divides between what is purely technical and what is social. The author builds "its" or "his" own text in such a way that the text itself is a machine that exemplifies several of the points made by the author. In particular, the author is constructed and deconstructed several times to show how many social actors are inscribed or prescribed by machines and automatisms.},
	number = {3},
	urldate = {2025-04-09},
	journal = {Social Problems},
	author = {Johnson, Jim},
	year = {1988},
	note = {Publisher: [Oxford University Press, Society for the Study of Social Problems]},
	pages = {298--310},
}

@article{chen_reasoning_2025-1,
	title = {Reasoning {Models} {Don}’t {Always} {Say} {What} {They} {Think}},
	abstract = {Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model’s CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models’ actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1\% of examples where they use the hint, but the reveal rate is often below 20\%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.},
	language = {en},
	author = {Chen, Yanda and Benton, Joe and Radhakrishnan, Ansh and Uesato, Jonathan and Denison, Carson and Schulman, John and Somani, Arushi and Hase, Peter and Wagner, Misha and Roger, Fabien and Mikulik, Vlad and Bowman, Sam and Leike, Jan and Kaplan, Jared and Perez, Ethan},
	year = {2025},
}

@misc{assink_making_2021,
	type = {info:eu-repo/semantics/{masterThesis}},
	title = {Making the {Invisible} {Visible}: {Exploring} {Gender} {Bias} in {AI} {Voice} {Assistants}},
	shorttitle = {Making the {Invisible} {Visible}},
	url = {https://essay.utwente.nl/88121/},
	abstract = {Background: Voice assistants are the future of technology interactions, but releasing predominantly female voices can reinforce subconscious gender biases and female stereotypes. A genderless voice assistant was developed to overcome any possible biases. However, due to its novelty, the effects of a genderless voice assistant have not been tested. Research aim: This research investigates if voice assistants with different gendered voices (female, male, genderless) affect the users’ perception of trust, attractiveness, and usability. Moreover, it is examined if the context of use moderates the effect of the voice assistant in the car, phone, and home. Furthermore, a moderating effect of the participant's age and gender is tested. Method: A 3x3 experimental design with nine video conditions of a voice assistant-human interaction, followed by a questionnaire. The gathered data contained 315 randomly selected participants. The data analysis technique encompassed a factor analysis and three MANOVAs. Results: No significant effects of the voice assistants’ gender on trust, attractiveness, and usability were found. Furthermore, there is no moderating effect of the context of use, nor a moderating effect for the participants’ age or gender. Conclusion: Since the results of the current study did not show an effect of the voice assistants’ gender on users’ perception of trust, attractiveness, or usability, it might be the case that male, female as well as genderless voices are interpreted the same by users. Hence, it is recommended to research and develop more genderless voice assistant alternatives to overcome gender biases and create a more inclusive future.},
	language = {en},
	urldate = {2025-02-06},
	author = {Assink, Lena Marie},
	month = aug,
	year = {2021},
	note = {Publisher: University of Twente},
}

@article{deng_people_2024,
	title = {Do {People} {Trust} {Female} more than {Male} as a {Voice} {Assistant}?},
	abstract = {The process of human-computer interaction invariably involves voice assistants (VAs) whose gender is a key factor in shaping user perceptions. Existing research into the gender effect of VAs’ voice on perceived credibility of the VAs remains ambiguous and inconclusive. The present study extends literature in this area with a systematic testing of the mediation effect of anthropomorphism, social presence, and the sense of enjoyment in this relationship. An online experiment randomly assigned 120 participants to either a male or female VA voice condition for health-related information. Results show that compared to a male voice, VAs with a female voice are perceived with a higher level of anthropomorphism, which in turn increases perceived credibility of the information. Both theoretical and practical implications of this study were discussed.},
	language = {en},
	author = {Deng, Dion and Zhang, Xinzhi and Guo, Steve},
	year = {2024},
}

@article{mullennix_social_2003,
	title = {Social perception of male and female computer synthesized speech},
	volume = {19},
	issn = {0747-5632},
	url = {https://www.sciencedirect.com/science/article/pii/S074756320200081X},
	doi = {10.1016/S0747-5632(02)00081-X},
	abstract = {The present study addressed the issue of whether social perception of human speech and computerized text-to-speech (TTS) is affected by gender of voice and gender of listener. Listeners were presented with a persuasive argument in either male or female human or synthetic voice and were assessed on attitude change and their ratings of various speech qualities. The results indicated that female human speech was rated as preferable to female synthetic speech, and that male synthetic speech was rated as preferable to female synthetic speech. Degree of persuasion did not differ across human and synthetic speech, however, female listeners were persuaded more by the argument than male listeners were. Patterns of ratings across male and female listeners were fairly similar across human and synthetic speech, suggesting that gender stereotyping for human voices and computerized voices may occur in a similar fashion.},
	number = {4},
	urldate = {2025-02-06},
	journal = {Computers in Human Behavior},
	author = {Mullennix, John W and Stern, Steven E and Wilson, Stephen J and Dyson, Corrie-lynn},
	month = jul,
	year = {2003},
	keywords = {Gender, Persuasion, Social influence, Synthetic speech, Text-to-speech (TTS)},
	pages = {407--424},
}

@book{mclaughlin_communication_2012,
	address = {New York},
	title = {Communication {Yearbook} 9},
	isbn = {978-0-203-85621-5},
	abstract = {The Communication Yearbook annuals publish diverse, state-of-the-discipline literature reviews that advance knowledge and understanding of communication systems, processes, and impacts across the discipline. Sponsored by the International Communication Association, each volume provides a forum for the exchange of interdisciplinary and internationally diverse scholarship relating to communication in its many forms. This volume re-issues the yearbook from 1986.},
	publisher = {Routledge},
	editor = {McLaughlin, Margaret},
	month = may,
	year = {2012},
	doi = {10.4324/9780203856215},
}

@article{lloyd_miami_2019,
	title = {Miami {University} deception detection database},
	volume = {51},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-018-1061-4},
	doi = {10.3758/s13428-018-1061-4},
	abstract = {In the present work, we introduce the Miami University Deception Detection Database (MU3D), a free resource containing 320 videos of target individuals telling truths and lies. Eighty (20 Black female, 20 Black male, 20 White female, and 20 White male) different targets were recorded speaking honestly and dishonestly about their social relationships. Each target generated four different videos (i.e., positive truth, negative truth, positive lie, negative lie), yielding 320 videos fully crossing target race, target gender, statement valence, and statement veracity. These videos were transcribed by trained research assistants and evaluated by naïve raters. Descriptive analyses of the video characteristics (e.g., length) and subjective ratings (e.g., target attractiveness) are provided. The stimuli and an information codebook can be accessed free of charge for academic research purposes from http://hdl.handle.net/2374.MIA/6067. The MU3D offers scholars the ability to conduct research using standardized stimuli that can aid in building more comprehensive theories of interpersonal sensitivity, enhance replication among labs, facilitate the use of signal detection analyses, and promote consideration of race, gender, and their interactive effects in deception detection research.},
	language = {en},
	number = {1},
	urldate = {2025-02-07},
	journal = {Behavior Research Methods},
	author = {Lloyd, E. Paige and Deska, Jason C. and Hugenberg, Kurt and McConnell, Allen R. and Humphrey, Brandon T. and Kunstman, Jonathan W.},
	month = feb,
	year = {2019},
	keywords = {Lie detection, Stimulus set, Video database},
	pages = {429--439},
}

@article{markowitz_when_2020,
	title = {When context matters: how false, truthful, and genre-related communication styles are revealed in language},
	volume = {26},
	issn = {1068-316X},
	shorttitle = {When context matters},
	url = {https://doi.org/10.1080/1068316X.2019.1652751},
	doi = {10.1080/1068316X.2019.1652751},
	abstract = {In this preregistered experiment, we address an understudied question in the deception and language literature: What is the impact of context on false and truthful language patterns? Drawing on two theories, Truth-Default Theory and the Contextual Organization of Language and Deception model, we instructed participants (N = 639) to lie, tell the truth, or write within a genre without explicit lying or truth-telling instructions across different topics (e.g. their friends, attitudes on abortion). The results successfully replicate several cue-based models for self-references and negative affect, such as the Newman Pennebaker model of deception. Participants without lying or truth-telling instructions, but who wrote within genre conventions, showed markedly similar patterns to truth-tellers, though indicators of analytic thinking, adjectives, and auxiliary verbs were distinct. The data were also evaluated with a topic modeling approach and suggest that the abortion process was construed negatively when people lied about the topic. Truth-tellers construed abortion in objective terms and genre-related speech highlighted key role-players (e.g. the government, men, women, baby). We discuss how these data advance deception and language theory.},
	number = {3},
	urldate = {2025-02-07},
	journal = {Psychology, Crime \& Law},
	author = {Markowitz, David M. and Griffin, Darrin J.},
	month = mar,
	year = {2020},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/1068316X.2019.1652751},
	keywords = {Deception, automated text analysis, context, genre, language},
	pages = {287--310},
}

@article{levine_truth-default_2014-1,
	title = {Truth-{Default} {Theory} ({TDT}): {A} {Theory} of {Human} {Deception} and {Deception} {Detection}},
	volume = {33},
	issn = {0261-927X},
	shorttitle = {Truth-{Default} {Theory} ({TDT})},
	url = {https://doi.org/10.1177/0261927X14535916},
	doi = {10.1177/0261927X14535916},
	abstract = {Truth-Default Theory (TDT) is a new theory of deception and deception detection. This article offers an initial sketch of, and brief introduction to, TDT. The theory seeks to provide an elegant explanation of previous findings as well as point to new directions for future research. Unlike previous theories of deception detection, TDT emphasizes contextualized communication content in deception detection over nonverbal behaviors associated with emotions, arousal, strategic self-presentation, or cognitive effort. The central premises of TDT are that people tend to believe others and that this “truth-default” is adaptive. Key definitions are provided. TDT modules and propositions are briefly explicated. Finally, research consistent with TDT is summarized.},
	language = {en},
	number = {4},
	urldate = {2025-02-07},
	journal = {Journal of Language and Social Psychology},
	author = {Levine, Timothy R.},
	month = sep,
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	pages = {378--392},
}

@book{levine_duped_2020,
	address = {Tuscaloosa},
	title = {Duped: truth-default theory and the social science of lying and deception},
	isbn = {978-0-8173-2041-6 978-0-8173-5968-3},
	shorttitle = {Duped},
	publisher = {The University of Alabama Press},
	author = {Levine, Timothy R.},
	year = {2020},
	keywords = {Deception, Social aspects, Truthfulness and falsehood},
}

@article{clare_documenting_2019,
	title = {Documenting the {Truth}-{Default}: {The} {Low} {Frequency} of {Spontaneous} {Unprompted} {Veracity} {Assessments} in {Deception} {Detection}},
	volume = {45},
	issn = {0360-3989},
	shorttitle = {Documenting the {Truth}-{Default}},
	url = {https://doi.org/10.1093/hcr/hqz001},
	doi = {10.1093/hcr/hqz001},
	abstract = {The core idea of truth-default theory (T. R. Levine, 2014) is that when people cognitively process the content of others’ communication, they typically do so in a manner characterized by unquestioned, passive acceptance. Two deception detection experiments tested the existence of the truth-default by comparing prompted and unprompted evaluations of others. The first experiment involved viewing videotaped communication, and the second experiment involved live, face-to-face interactions. In both experiments, research confederates told the truth and lied about plausible and implausible autobiographical content. Participants completed both traditional, prompted, dichotomous truth-lie assessments and open-ended thought-listing measures. The order of the two types of measures was experimentally varied. The results supported the concept of a truth-default. Coded thought listings showed that, absent prior prompting, receivers mentioned consideration of the veracity of other’s communication less than 10\% of the time.},
	number = {3},
	urldate = {2025-02-07},
	journal = {Human Communication Research},
	author = {Clare, David D and Levine, Timothy R},
	month = jul,
	year = {2019},
	pages = {286--308},
}

@misc{ott_finding_2011,
	title = {Finding {Deceptive} {Opinion} {Spam} by {Any} {Stretch} of the {Imagination}},
	url = {http://arxiv.org/abs/1107.4557},
	doi = {10.48550/arXiv.1107.4557},
	abstract = {Consumers increasingly rate, review and research products online. Consequently, websites containing consumer reviews are becoming targets of opinion spam. While recent work has focused primarily on manually identifiable instances of opinion spam, in this work we study deceptive opinion spam---fictitious opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90\% accurate on our gold-standard opinion spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing.},
	urldate = {2025-02-07},
	publisher = {arXiv},
	author = {Ott, Myle and Choi, Yejin and Cardie, Claire and Hancock, Jeffrey T.},
	month = jul,
	year = {2011},
	note = {arXiv:1107.4557 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{openai_openai_nodate,
	title = {{OpenAI} {Platform}},
	url = {https://platform.openai.com},
	abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
	language = {en},
	urldate = {2025-02-07},
	author = {OpenAI},
}

@article{hoogland_o1_2024,
	title = {o1: {A} {Technical} {Primer}},
	shorttitle = {o1},
	url = {https://www.lesswrong.com/posts/byNYzsfFmb2TpYFPW/o1-a-technical-primer},
	abstract = {{\textgreater} TL;DR: In September 2024, OpenAI released o1, its first "reasoning model". This model exhibits remarkable test-time scaling laws, which complete a…},
	language = {en},
	urldate = {2025-02-07},
	author = {Hoogland, Jesse},
	month = dec,
	year = {2024},
}

@misc{openai_learning_nodate,
	title = {Learning to reason with {LLMs}},
	url = {https://openai.com/index/learning-to-reason-with-llms/},
	abstract = {We are introducing OpenAI o1, a new large language model trained with reinforcement learning to perform complex reasoning. o1 thinks before it answers—it can produce a long internal chain of thought before responding to the user.},
	language = {en-US},
	urldate = {2025-02-07},
	author = {OpenAI},
}

@article{depaulo_cues_2003-1,
	title = {Cues to deception.},
	volume = {129},
	issn = {1939-1455, 0033-2909},
	url = {https://doi.apa.org/doi/10.1037/0033-2909.129.1.74},
	doi = {10.1037/0033-2909.129.1.74},
	language = {en},
	number = {1},
	urldate = {2025-02-07},
	journal = {Psychological Bulletin},
	author = {DePaulo, Bella M. and Lindsay, James J. and Malone, Brian E. and Muhlenbruck, Laura and Charlton, Kelly and Cooper, Harris},
	year = {2003},
	pages = {74--118},
}

@article{hartwig_why_2011,
	title = {Why do lie-catchers fail? {A} lens model meta-analysis of human lie judgments.},
	volume = {137},
	issn = {1939-1455, 0033-2909},
	shorttitle = {Why do lie-catchers fail?},
	url = {https://doi.apa.org/doi/10.1037/a0023589},
	doi = {10.1037/a0023589},
	abstract = {Decades of research has shown that people are poor at detecting lies. Two explanations for this finding have been proposed. First, it has been suggested that lie detection is inaccurate because people rely on invalid cues when judging deception. Second, it has been suggested that lack of valid cues to deception limits accuracy. A series of 4 meta-analyses tested these hypotheses with the framework of Brunswik’s (1952) lens model. Meta-Analysis 1 investigated perceived cues to deception by correlating 66 behavioral cues in 153 samples with deception judgments. People strongly associate deception with impressions of incompetence (r ϭ .59) and ambivalence (r ϭ .49). Contrary to self-reports, eye contact is only weakly correlated with deception judgments (r ϭ Ϫ.15). Cues to perceived deception were then compared with cues to actual deception. The results show a substantial covariation between the 2 sets of cues (r ϭ .59 in Meta-Analysis 2, r ϭ .72 in Meta-Analysis 3). Finally, in Meta-Analysis 4, a lens model analysis revealed a very strong matching between behaviorally based predictions of deception and behaviorally based predictions of perceived deception. In conclusion, contrary to previous assumptions, people rarely rely on the wrong cues. Instead, limitations in lie detection accuracy are mainly attributable to weaknesses in behavioral cues to deception. The results suggest that intuitive notions about deception are more accurate than explicit knowledge and that lie detection is more readily improved by increasing behavioral differences between liars and truth tellers than by informing lie-catchers of valid cues to deception.},
	language = {en},
	number = {4},
	urldate = {2025-02-07},
	journal = {Psychological Bulletin},
	author = {Hartwig, Maria and Bond, Charles F.},
	year = {2011},
	pages = {643--659},
}

@misc{noauthor_lessons_nodate,
	title = {Lessons {From} {Pinocchio}: {Cues} to {Deception} {May} {Be} {Highly} {Exaggerated} - {Timothy} {J}. {Luke}, 2019},
	url = {https://journals.sagepub.com/doi/10.1177/1745691619838258},
	urldate = {2025-02-07},
}

@misc{muennighoff_s1_2025,
	title = {s1: {Simple} test-time scaling},
	shorttitle = {s1},
	url = {http://arxiv.org/abs/2501.19393},
	doi = {10.48550/arXiv.2501.19393},
	abstract = {Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending "Wait" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27\% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50\% to 57\% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1},
	urldate = {2025-02-08},
	publisher = {arXiv},
	author = {Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Candès, Emmanuel and Hashimoto, Tatsunori},
	month = feb,
	year = {2025},
	note = {arXiv:2501.19393 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{luke_lessons_2019,
	title = {Lessons {From} {Pinocchio}: {Cues} to {Deception} {May} {Be} {Highly} {Exaggerated}},
	abstract = {Deception researchers widely acknowledge that cues to deception—observable behaviors that may differ between truthful and deceptive messages—tend to be weak. Nevertheless, several deception cues have been reported with unusually large effect sizes, and some researchers have advocated the use of such cues as tools for detecting deceit and assessing credibility in practical contexts. By examining data from empirical deception-cue research and using a series of Monte Carlo simulations, I demonstrate that many estimated effect sizes of deception cues may be greatly inflated by publication bias, small numbers of estimates, and low power. Indeed, simulations indicate the informational value of the present deception literature is quite low, such that it is not possible to determine whether any given effect is real or a false positive. I warn against the hazards of relying on potentially illusory cues to deception and offer some recommendations for improving the state of the science of deception.},
	language = {en},
	author = {Luke, Timothy J},
	year = {2019},
}

@misc{noauthor_generative_nodate,
	title = {Generative {AI} and the politics of visibility},
	url = {https://journals.sagepub.com/doi/epub/10.1177/20539517241252131},
	language = {en},
	urldate = {2025-02-08},
	doi = {10.1177/20539517241252131},
}

@misc{ott_finding_2011-1,
	title = {Finding {Deceptive} {Opinion} {Spam} by {Any} {Stretch} of the {Imagination}},
	url = {http://arxiv.org/abs/1107.4557},
	doi = {10.48550/arXiv.1107.4557},
	abstract = {Consumers increasingly rate, review and research products online. Consequently, websites containing consumer reviews are becoming targets of opinion spam. While recent work has focused primarily on manually identifiable instances of opinion spam, in this work we study deceptive opinion spam---fictitious opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90\% accurate on our gold-standard opinion spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing.},
	urldate = {2025-02-25},
	publisher = {arXiv},
	author = {Ott, Myle and Choi, Yejin and Cardie, Claire and Hancock, Jeffrey T.},
	month = jul,
	year = {2011},
	note = {arXiv:1107.4557 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@book{nass_wired_2007,
	address = {Cambridge, Mass.},
	title = {Wired for {Speech}: {How} {Voice} {Activates} and {Advances} the {Human}-{Computer} {Relationship}},
	isbn = {978-0-262-64065-7},
	shorttitle = {Wired for {Speech}},
	abstract = {How interactive voice-based technology can tap into the automatic and powerful responses all speech—whether from human or machine—evokes.Interfaces that talk and listen are populating computers, cars, call centers, and even home appliances and toys, but voice interfaces invariably frustrate rather than help. In Wired for Speech, Clifford Nass and Scott Brave reveal how interactive voice technologies can readily and effectively tap into the automatic responses all speech—whether from human or machine—evokes. Wired for Speech demonstrates that people are "voice-activated": we respond to voice technologies as we respond to actual people and behave as we would in any social situation. By leveraging this powerful finding, voice interfaces can truly emerge as the next frontier for efficient, user-friendly technology.Wired for Speech presents new theories and experiments and applies them to critical issues concerning how people interact with technology-based voices. It considers how people respond to a female voice in e-commerce (does stereotyping matter?), how a car's voice can promote safer driving (are "happy" cars better cars?), whether synthetic voices have personality and emotion (is sounding like a person always good?), whether an automated call center should apologize when it cannot understand a spoken request ("To Err is Interface; To Blame, Complex"), and much more. Nass and Brave's deep understanding of both social science and design, drawn from ten years of research at Nass's Stanford laboratory, produces results that often challenge conventional wisdom and common design practices. These insights will help designers and marketers build better interfaces, scientists construct better theories, and everyone gain better understandings of the future of the machines that speak with us.},
	language = {English},
	publisher = {MIT Press},
	author = {Nass, Clifford and Brave, Scott},
	month = feb,
	year = {2007},
}

@misc{openai_reasoning_2025,
	title = {Reasoning models - {OpenAI} {API}},
	url = {https://platform.openai.com},
	abstract = {Explore the capabilities of OpenAI's o1 series for complex reasoning and problem-solving. Learn about their features and how they compare to GPT-4o models.},
	language = {en},
	urldate = {2025-03-14},
	author = {{OpenAI}},
	year = {2025},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{openai_openai_2024,
	title = {{OpenAI} o1 {System} {Card}},
	url = {https://cdn.openai.com/o1-system-card-20241205.pdf},
	author = {{OpenAI}},
	month = dec,
	year = {2024},
}

@misc{anthropic_claude_2025,
	title = {Claude 3.7 {Sonnet} {System} {Card}},
	author = {{Anthropic}},
	month = feb,
	year = {2025},
}

@article{fang_how_2025,
	title = {{HOW} {AI} {AND} {HUMAN} {BEHAVIORS} {SHAPE} {PSYCHOSOCIAL} {EFFECTS} {OF} {CHATBOT} {USE}: {A} {LONGITUDINAL} {RANDOMIZED} {CONTROLLED} {STUDY}},
	abstract = {AI chatbots, especially those with voice capabilities, have become increasingly human-like, with more users seeking emotional support and companionship from them. Concerns are rising about how such interactions might impact users’ loneliness and socialization with real people. We conducted a four-week randomized, controlled, IRB-approved experiment (n=981, {\textgreater}300K messages) to investigate how AI chatbot interaction modes (text, neutral voice, and engaging voice) and conversation types (open-ended, non-personal, and personal) influence psychosocial outcomes such as loneliness, social interaction with real people, emotional dependence on AI and problematic AI usage. Results showed that while voice-based chatbots initially appeared beneficial in mitigating loneliness and dependence compared with text-based chatbots, these advantages diminished at high usage levels, especially with a neutral-voice chatbot. Conversation type also shaped outcomes: personal topics slightly increased loneliness but tended to lower emotional dependence compared with open-ended conversations, whereas non-personal topics were associated with greater dependence among heavy users. Overall, higher daily usage—across all modalities and conversation types—correlated with higher loneliness, dependence, and problematic use, and lower socialization. Exploratory analyses revealed that those with stronger emotional attachment tendencies and higher trust in the AI chatbot tended to experience greater loneliness and emotional dependence, respectively. These findings underscore the complex interplay between chatbot design choices (e.g., voice expressiveness) and user behaviors (e.g., conversation content, usage frequency). We highlight the need for further research on whether chatbots’ ability to manage emotional content without fostering dependence or replacing human relationships benefits overall well-being.},
	language = {en},
	author = {Fang, Cathy Mengying and Liu, Auren R and Danry, Valdemar and Lee, Eunhae and Chan, Samantha W T and Pataranutaporn, Pat and Maes, Pattie and Phang, Jason and Lampe, Michael and Ahmad, Lama and Agarwal, Sandhini},
	year = {2025},
}

@inproceedings{bhattacharya_switching_2024,
	title = {Switching {Tongues}, {Sharing} {Hearts}: {Identifying} the {Relationship} between {Empathy} and {Code}-switching in {Speech}},
	shorttitle = {Switching {Tongues}, {Sharing} {Hearts}},
	url = {https://www.isca-archive.org/interspeech_2024/bhattacharya24_interspeech.html},
	doi = {10.21437/Interspeech.2024-1224},
	abstract = {Among the many multilingual speakers of the world, codeswitching (CSW) is a common linguistic phenomenon. Prior sociolinguistic work has shown that factors such as expressing group identity and solidarity, performing affective function, and reflecting shared experiences are related to CSW prevalence in multilingual speech. We build on prior studies by asking: is the expression of empathy a motivation for CSW in speech? To begin to answer this question, we examine several multilingual speech corpora representing diverse language families and apply recent modeling advances in the study of empathetic monolingual speech. We find a generally stronger positive relationship of spoken CSW with the lexical correlates of empathy than with acoustic-prosodic ones, which holds across three language pairs. Our work is a first step toward establishing a motivation for CSW that has thus far mainly been studied qualitatively.},
	language = {en},
	urldate = {2025-03-22},
	booktitle = {Interspeech 2024},
	publisher = {ISCA},
	author = {Bhattacharya, Debasmita and Lin, Eleanor and Chen, Run and Hirschberg, Julia},
	month = sep,
	year = {2024},
	pages = {492--496},
}

@article{eloundou_first-person_nodate,
	title = {First-{Person} {Fairness} in {Chatbots}},
	abstract = {Chatbots like ChatGPT are used by hundreds of millions of people for diverse purposes, ranging from r´esum´e writing to entertainment. These real-world applications are different from the institutional uses, such as r´esum´e screening or credit scoring, which have been the focus of much of AI research on bias and fairness. Ensuring equitable treatment for all users in these first-person contexts is critical. In this work, we study “first-person fairness,” which means fairness toward the user who is interacting with a chatbot. This includes providing high-quality responses to all users regardless of their identity or background, and avoiding harmful stereotypes. We propose a scalable, privacy-preserving method for evaluating one aspect of first-person fairness across a large, heterogeneous corpus of real-world chatbot interactions. Specifically, we assess potential bias linked to users’ names, which can serve as proxies for demographic attributes like gender or race, in chatbot systems such as ChatGPT, which provide mechanisms for storing and using user names. Our method leverages a second language model to privately analyze name-sensitivity in the chatbot’s responses. We verify the validity of these annotations through independent human evaluation. Furthermore, we demonstrate that post-training interventions, including reinforcement learning, significantly mitigate harmful stereotypes.},
	language = {en},
	author = {Eloundou, Tyna and Beutel, Alex and Robinson, David G and Gu-Lemberg, Keren and Brakman, Anna-Luisa and Mishkin, Pamela and Shah, Meghan and Heidecke, Johannes and Weng, Lilian and Kalai, Adam Tauman},
}

@article{phang_investigating_nodate,
	title = {Investigating {Aﬀective} {Use} and {Emotional} {Well}-being on {ChatGPT}},
	abstract = {As AI chatbots see increased adoption and integration into everyday life, questions have been raised about the potential impact of human-like or anthropomorphic AI on users. In this work, we investigate the extent to which interactions with ChatGPT (with a focus on Advanced Voice Mode) may impact users’ emotional well-being, behaviors and experiences through two parallel studies. To study the aﬀective use of AI chatbots, we perform large-scale automated analysis of ChatGPT platform usage in a privacy-preserving manner, analyzing over 4 million conversations for aﬀective cues and surveying over 4,000 users on their perceptions of ChatGPT. To investigate whether there is a relationship between model usage and emotional well-being, we conduct an Institutional Review Board (IRB)-approved randomized controlled trial (RCT) on close to 1,000 participants over 28 days, examining changes in their emotional well-being as they interact with ChatGPT under diﬀerent experimental settings. In both on-platform data analysis and the RCT, we observe that very high usage correlates with increased self-reported indicators of dependence. From our RCT, we ﬁnd that the impact of voice-based interactions on emotional well-being to be highly nuanced, and inﬂuenced by factors such as the user’s initial emotional state and total usage duration. Overall, our analysis reveals that a small number of users are responsible for a disproportionate share of the most aﬀective cues.},
	language = {en},
	author = {Phang, Jason and Lampe, Michael and Ahmad, Lama and Agarwal, Sandhini and Fang, Cathy Mengying and Liu, Auren R and Danry, Valdemar and Lee, Eunhae and Chan, Samantha W T and Pataranutaporn, Pat and Maes, Pattie},
}

@misc{bai_ai-generated_2025,
	title = {{AI}-{Generated} {Messages} {Can} {Be} {Used} to {Persuade} {Humans} on {Policy} {Issues}},
	url = {https://osf.io/stakv_v5},
	doi = {10.31219/osf.io/stakv_v5},
	abstract = {The emergence of large language models (LLMs) has made it possible for generative artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets in the U.S. and globally. Here, we investigate whether existing, openly-available LLMs can be used to create messages capable of influencing humans’ political attitudes. Across three pre-registered experiments (total N = 4,829), we find consistent evidence that assigning participants to read persuasive messages generated by LLMs can lead to attitude change across a range of policies, including highly polarized policies, such as an assault weapons ban, a carbon tax, and a paid parental-leave program. Overall, we found LLM-generated messages were similarly effective in influencing policy attitudes as were messages crafted by lay humans. These results demonstrate that recent developments in AI make it possible to create politically persuasive messages cheaply and at massive scale.},
	language = {en-us},
	urldate = {2025-03-30},
	publisher = {OSF},
	author = {Bai, (Max) Hui and Voelkel, Jan G. and Muldowney, Shane and Eichstaedt, johannes C. and Willer, Robb},
	month = mar,
	year = {2025},
}

@article{markowitz_generative_2024,
	title = {Generative {AI} {Are} {More} {Truth}-{Biased} {Than} {Humans}: {A} {Replication} and {Extension} of {Core} {Truth}-{Default} {Theory} {Principles}},
	volume = {43},
	issn = {0261-927X, 1552-6526},
	shorttitle = {Generative {AI} {Are} {More} {Truth}-{Biased} {Than} {Humans}},
	url = {https://journals.sagepub.com/doi/10.1177/0261927X231220404},
	doi = {10.1177/0261927X231220404},
	abstract = {Humans often display a truth-bias—the perception that others are honest independent of message veracity—but does this phenomenon extend to generative artiﬁcial intelligence (AI)? We had humans and large language models make nearly 1,000 veracity judgments across different prompts. Human detection accuracies were near chance (50\%–53\%) with notable truth-biases (59\%–64\%); AI had a substantially greater truth-bias than humans (67\%–99\%). GPT-4 was also truth-default, not suspecting deception when veracity assessments were unprompted. Together, people and AI judge most information to be true.},
	language = {en},
	number = {2},
	urldate = {2025-02-05},
	journal = {Journal of Language and Social Psychology},
	author = {Markowitz, David M. and Hancock, Jeffrey T.},
	month = mar,
	year = {2024},
	pages = {261--267},
}

@misc{openai2024gpt4ocard,
      title={GPT-4o System Card}, 
      author={OpenAI and : and Aaron Hurst and Adam Lerer and Adam P. Goucher and Adam Perelman and Aditya Ramesh and Aidan Clark and AJ Ostrow and Akila Welihinda and Alan Hayes and Alec Radford and Aleksander Mądry and Alex Baker-Whitcomb and Alex Beutel and Alex Borzunov and Alex Carney and Alex Chow and Alex Kirillov and Alex Nichol and Alex Paino and Alex Renzin and Alex Tachard Passos and Alexander Kirillov and Alexi Christakis and Alexis Conneau and Ali Kamali and Allan Jabri and Allison Moyer and Allison Tam and Amadou Crookes and Amin Tootoochian and Amin Tootoonchian and Ananya Kumar and Andrea Vallone and Andrej Karpathy and Andrew Braunstein and Andrew Cann and Andrew Codispoti and Andrew Galu and Andrew Kondrich and Andrew Tulloch and Andrey Mishchenko and Angela Baek and Angela Jiang and Antoine Pelisse and Antonia Woodford and Anuj Gosalia and Arka Dhar and Ashley Pantuliano and Avi Nayak and Avital Oliver and Barret Zoph and Behrooz Ghorbani and Ben Leimberger and Ben Rossen and Ben Sokolowsky and Ben Wang and Benjamin Zweig and Beth Hoover and Blake Samic and Bob McGrew and Bobby Spero and Bogo Giertler and Bowen Cheng and Brad Lightcap and Brandon Walkin and Brendan Quinn and Brian Guarraci and Brian Hsu and Bright Kellogg and Brydon Eastman and Camillo Lugaresi and Carroll Wainwright and Cary Bassin and Cary Hudson and Casey Chu and Chad Nelson and Chak Li and Chan Jun Shern and Channing Conger and Charlotte Barette and Chelsea Voss and Chen Ding and Cheng Lu and Chong Zhang and Chris Beaumont and Chris Hallacy and Chris Koch and Christian Gibson and Christina Kim and Christine Choi and Christine McLeavey and Christopher Hesse and Claudia Fischer and Clemens Winter and Coley Czarnecki and Colin Jarvis and Colin Wei and Constantin Koumouzelis and Dane Sherburn and Daniel Kappler and Daniel Levin and Daniel Levy and David Carr and David Farhi and David Mely and David Robinson and David Sasaki and Denny Jin and Dev Valladares and Dimitris Tsipras and Doug Li and Duc Phong Nguyen and Duncan Findlay and Edede Oiwoh and Edmund Wong and Ehsan Asdar and Elizabeth Proehl and Elizabeth Yang and Eric Antonow and Eric Kramer and Eric Peterson and Eric Sigler and Eric Wallace and Eugene Brevdo and Evan Mays and Farzad Khorasani and Felipe Petroski Such and Filippo Raso and Francis Zhang and Fred von Lohmann and Freddie Sulit and Gabriel Goh and Gene Oden and Geoff Salmon and Giulio Starace and Greg Brockman and Hadi Salman and Haiming Bao and Haitang Hu and Hannah Wong and Haoyu Wang and Heather Schmidt and Heather Whitney and Heewoo Jun and Hendrik Kirchner and Henrique Ponde de Oliveira Pinto and Hongyu Ren and Huiwen Chang and Hyung Won Chung and Ian Kivlichan and Ian O'Connell and Ian O'Connell and Ian Osband and Ian Silber and Ian Sohl and Ibrahim Okuyucu and Ikai Lan and Ilya Kostrikov and Ilya Sutskever and Ingmar Kanitscheider and Ishaan Gulrajani and Jacob Coxon and Jacob Menick and Jakub Pachocki and James Aung and James Betker and James Crooks and James Lennon and Jamie Kiros and Jan Leike and Jane Park and Jason Kwon and Jason Phang and Jason Teplitz and Jason Wei and Jason Wolfe and Jay Chen and Jeff Harris and Jenia Varavva and Jessica Gan Lee and Jessica Shieh and Ji Lin and Jiahui Yu and Jiayi Weng and Jie Tang and Jieqi Yu and Joanne Jang and Joaquin Quinonero Candela and Joe Beutler and Joe Landers and Joel Parish and Johannes Heidecke and John Schulman and Jonathan Lachman and Jonathan McKay and Jonathan Uesato and Jonathan Ward and Jong Wook Kim and Joost Huizinga and Jordan Sitkin and Jos Kraaijeveld and Josh Gross and Josh Kaplan and Josh Snyder and Joshua Achiam and Joy Jiao and Joyce Lee and Juntang Zhuang and Justyn Harriman and Kai Fricke and Kai Hayashi and Karan Singhal and Katy Shi and Kavin Karthik and Kayla Wood and Kendra Rimbach and Kenny Hsu and Kenny Nguyen and Keren Gu-Lemberg and Kevin Button and Kevin Liu and Kiel Howe and Krithika Muthukumar and Kyle Luther and Lama Ahmad and Larry Kai and Lauren Itow and Lauren Workman and Leher Pathak and Leo Chen and Li Jing and Lia Guy and Liam Fedus and Liang Zhou and Lien Mamitsuka and Lilian Weng and Lindsay McCallum and Lindsey Held and Long Ouyang and Louis Feuvrier and Lu Zhang and Lukas Kondraciuk and Lukasz Kaiser and Luke Hewitt and Luke Metz and Lyric Doshi and Mada Aflak and Maddie Simens and Madelaine Boyd and Madeleine Thompson and Marat Dukhan and Mark Chen and Mark Gray and Mark Hudnall and Marvin Zhang and Marwan Aljubeh and Mateusz Litwin and Matthew Zeng and Max Johnson and Maya Shetty and Mayank Gupta and Meghan Shah and Mehmet Yatbaz and Meng Jia Yang and Mengchao Zhong and Mia Glaese and Mianna Chen and Michael Janner and Michael Lampe and Michael Petrov and Michael Wu and Michele Wang and Michelle Fradin and Michelle Pokrass and Miguel Castro and Miguel Oom Temudo de Castro and Mikhail Pavlov and Miles Brundage and Miles Wang and Minal Khan and Mira Murati and Mo Bavarian and Molly Lin and Murat Yesildal and Nacho Soto and Natalia Gimelshein and Natalie Cone and Natalie Staudacher and Natalie Summers and Natan LaFontaine and Neil Chowdhury and Nick Ryder and Nick Stathas and Nick Turley and Nik Tezak and Niko Felix and Nithanth Kudige and Nitish Keskar and Noah Deutsch and Noel Bundick and Nora Puckett and Ofir Nachum and Ola Okelola and Oleg Boiko and Oleg Murk and Oliver Jaffe and Olivia Watkins and Olivier Godement and Owen Campbell-Moore and Patrick Chao and Paul McMillan and Pavel Belov and Peng Su and Peter Bak and Peter Bakkum and Peter Deng and Peter Dolan and Peter Hoeschele and Peter Welinder and Phil Tillet and Philip Pronin and Philippe Tillet and Prafulla Dhariwal and Qiming Yuan and Rachel Dias and Rachel Lim and Rahul Arora and Rajan Troll and Randall Lin and Rapha Gontijo Lopes and Raul Puri and Reah Miyara and Reimar Leike and Renaud Gaubert and Reza Zamani and Ricky Wang and Rob Donnelly and Rob Honsby and Rocky Smith and Rohan Sahai and Rohit Ramchandani and Romain Huet and Rory Carmichael and Rowan Zellers and Roy Chen and Ruby Chen and Ruslan Nigmatullin and Ryan Cheu and Saachi Jain and Sam Altman and Sam Schoenholz and Sam Toizer and Samuel Miserendino and Sandhini Agarwal and Sara Culver and Scott Ethersmith and Scott Gray and Sean Grove and Sean Metzger and Shamez Hermani and Shantanu Jain and Shengjia Zhao and Sherwin Wu and Shino Jomoto and Shirong Wu and Shuaiqi and Xia and Sonia Phene and Spencer Papay and Srinivas Narayanan and Steve Coffey and Steve Lee and Stewart Hall and Suchir Balaji and Tal Broda and Tal Stramer and Tao Xu and Tarun Gogineni and Taya Christianson and Ted Sanders and Tejal Patwardhan and Thomas Cunninghman and Thomas Degry and Thomas Dimson and Thomas Raoux and Thomas Shadwell and Tianhao Zheng and Todd Underwood and Todor Markov and Toki Sherbakov and Tom Rubin and Tom Stasi and Tomer Kaftan and Tristan Heywood and Troy Peterson and Tyce Walters and Tyna Eloundou and Valerie Qi and Veit Moeller and Vinnie Monaco and Vishal Kuo and Vlad Fomenko and Wayne Chang and Weiyi Zheng and Wenda Zhou and Wesam Manassra and Will Sheu and Wojciech Zaremba and Yash Patil and Yilei Qian and Yongjik Kim and Youlong Cheng and Yu Zhang and Yuchen He and Yuchen Zhang and Yujia Jin and Yunxing Dai and Yury Malkov},
      year={2024},
      eprint={2410.21276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21276}, 
}
@misc{deepmindGemini,
	author = {Google},
	title = {{G}emini 2.5 {P}ro --- deepmind.google},
	howpublished = {\url{https://deepmind.google/technologies/gemini/pro/}},
	year = {},
	note = {[Accessed 10-04-2025]},
}
@misc{deepseekai2025deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
      year={2025},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}