\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025


% ready for submission
%\usepackage{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\title{Evaluating Truth-Bias in Reasoning AI Models}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Emilio ~Barkett\\
  Columbia University\\
  \texttt{eab2291@columbia.edu} \\
  % examples of more authors
   \And
   Madhavendra Thakur \\
   Columbia University \\
  % Address \\
   \texttt{mt3890@columbia.edu} \\
   %\And
   %Trisha Srikanth \\
   %Columbia University \\
  % Address \\
   %\texttt{trs2170@columbia.edu} \\
   \AND
   Olivia Long \\
   Columbia University \\
  % Address \\
   \texttt{ol2256@columbia.edu} \\
   \And
   Parker Anderson \\
   Columbia University \\
  % Address \\
   \texttt{pra2120@columbia.edu} \\
   \And
   Sunny Liu \\
   Columbia University \\
  % Address \\
   \texttt{xl3152@columbia.edu} \\
}

\begin{document}


\maketitle


\begin{abstract}
  Non-reasoning large language models (LLMs) have been shown to exhibit a stronger truth-bias than humans \citep{markowitz_generative_2024}, but does this extend to reasoning models? We had LLMs make nearly 600 veracity judgments across different prompts. Non-reasoning model detection accuracies were [substantially more truth-bias than reasoning models] (XX\%-XX\%); reasoning model detection accuracies were [marginally lower than non-reasoning models] (XX\%-XX\%). Together, both non-reasoning and reasoning LLMs remain more truth-bias than humans.
\end{abstract}

\section{Introduction}

%%%%%%% see Jacob Steinhardt's ``advice for authors'' on the abstract format. i think we can do better in making the first paragraph more concise. third sentence gets to our point?? or stick to fourth paragraph?

The \textit{truth-bias}---the idea that the receiver infers a message honest, independent of its veracity---is one of the most replicated findings in deception research \citep{levine_duped_2020, levine_truth-default_2014}. Its pervasiveness in humans has warranted the investigation into nonhuman judges, specifically generative artificial intelligence (AI) like large language models (LLMs). While previous work demonstrated the truth-bias in LLMs at 67\%-99\%, showing that AI judges most information to be true \citep{markowitz_generative_2024}, it focused its evaluation on non-reasoning LLMs, models that simply predict the next token, unlike reasoning LLMs, which are fine-tuned to exhibit step-by-step reasoning and to `think'. In the present study, we evaluate whether truth-bias in reasoning LLMs exhibits a comparable truth-bias to that of non-reasoning LLMs.

This study is relevant for several reasons. First, it tests the assumption that reasoning models \textit{should} outperform non-reasoning models on deception detection---a cognitively demanding task that requires deliberation---since reasoning models are capable of `thinking' rather than merely predicting the next token. Understanding whether reasoning models are more susceptible to truth-bias than non-reasoning models is crucial for evaluating their reliability and determining their suitability for epistemically demanding tasks. Second, it tests the assumption that current models will outperform previous models. While true in domains like generating code, images, or coherent text [CITE], we test the model's discernment of what is truthful and deceptive. Third, the results highlight the danger of coupling the persuasive abilities of LLMs with the truth-bias; there is a concerning risk that users could be nudged toward accepting deceptive beliefs. Finally, it offers a replicable method that can serve as a model evaluation for tracking the developmental progress of LLMs.

%%%%%Already, model system cards show that models are highly capable of persuasive communication \citep{anthropic_claude_2024, openai_openai_2024}. While GPT-3.5 demonstrated persuasive argumentative abilities in the 38\% percentile of humans, GPT-4o, o1, o1-preview, and o1-mini all score within the top ~80-90\% \citep{openai_openai_2024}.

%%% is there literature that shows that given more time, a human can detect deception with a higher degree of accuracy or is it always `near chance' at ~50\%.

Although an experimental design to evaluate a model's truth-bias is simple, it remains substantially more difficult to understand \textit{why} this bias arises and \textit{what} internal mechanisms are responsible. While answers to these questions fall outside the scope of the present study, we aim to document the truth-bias in non-reasoning and reasoning LLMs.

This study uses a dataset of deceptive and truthful hotel reviews \citep{ott_finding_2011} to elicit deception detection from LLMs \citep{markowitz_generative_2024}. Across four studies, we comparatively test the truth-bias in non-reasoning and reasoning LLMs. Study 1-3 solicited veracity statements, and Study 4 used an unprompted veracity statement. Non-reasoning model detection accuracies were [substantially more truth-bias than reasoning models] (XX\%-XX\%); reasoning model detection accuracies were [marginally lower than non-reasoning models] (XX\%-XX\%). Together, both non-reasoning and reasoning LLMs remain more truth-bias than humans.

We propose that reasoning models exhibit reduced truth-bias because reasoning processes emulate a form of reflective cognition that allows the model to evaluate statements more analytically. Unlike non-reasoning models, which predict the next token and may rely on surface-level associations, reasoning models are prompted to ``slow down'' via intermediate inference steps. This additional stepwise generation interrupts the tendency to default to truth-aligned completions, attenuating the truth-bias. We call this the \textit{Reflective Mediation Theory}, which holds that prompting LLMs to reason step-by-step introduces a deliberative process that disrupts default generative pathways, thereby mitigating the model’s tendency toward truth-default responses.

\section{Related Work}

This literature review will unfold as follows:

2.1 – Defining deception and truth-bias \& exploring societal implications \newline
2.2 – Truth-bias in humans \newline
2.3 – Human cognitive biases demonstrated in generative AI \newline
2.4 – Truth-bias in generative AI \newline
2.5 – Deception in reasoning AI models \newline

\textbf{2.1 – Defining deception and truth-bias \& exploring societal implications}

Deception is the intentional misleading of another person, and those who wish to deceive are aided by a propensity towards the truth-default state. In truth-default theory (TDT), those who are being deceived may default to passively assuming honesty – either the possibility of being deceived was not realized, or insufficient evidence to support potential deceit was gathered. [cite here] Truth-default leaves us exceptionally vulnerable to falsehoods: unless one actively perceives an interaction as suspicious, one is likely to believe in its honesty.

A closely related sibling of truth-default is truth-bias. Truth-bias is a well-studied phenomenon that is marked by a tendency to believe, regardless of actual veracity, in the truthfulness of statements made by conversational partners. Quantitatively, typical truth-bias experimental setups involve asking participants to judge the veracity of statements and counting the proportion of correct answers (see Section 2.2). For perspective, studies on truth-bias first started cropping up in the 1980s \citep{mccornack_deception_1986}, and TDT as described above is a relatively recent theory propositioned by Timothy R. Levine in 2014. 

Truth-default theory is important to mention since it is a potential criticism of human truth-bias experimental setups, as it underscores the influence of experimental settings (i.e: a laboratory) on truth-bias rates. In particular, if TDT were to hold in human experiments, truth bias rates would be much higher, as outside the experimental setting, the rate of deception is smaller; therefore, believing in someone's honesty would usually be correct.\citep{levine_truth-default_2014} However, for the purposes of our experiment (see Methodology), we are making API calls to a reasoning model, and as such, there are no situational dependencies as implied by TDT – unlike humans, AI models do not necessarily “know” that they are being tested on. \newline

\textbf{2.2 – Truth-bias in humans}

@Sunny

\textbf{2.3 – Human cognitive biases demonstrated in generative AI}

In this section, we describe a few human-like biases that AI models – specifically, generative AIs – exhibit. It is important to mention that many of the studies covered below only experiment with ChatGPT, and as such, their respective results cannot necessarily be generalized to other models. However, given the fact that all models are trained in bias-inducing ways (see 2.3.4), it seems reasonable to expect that generative AI tends to display human-like cognitive biases. While these biases are not necessarily similar to truth-bias, we intend to show that truth-bias is just one example of a cognitive bias that AI models have inherited. Additionally, some of these studies have acted as sources of inspiration for our own experimental design. In the interest of being succinct, we have omitted many experimental details. Please refer to the original studies to learn more.

\textit{2.3.1 Content biases}

LLMs have been found to perpetuate bias in the same way humans do: emphasizing “cognitively appealing and not necessarily informative, or valuable, content.” One such study conducted by Acerbi and Stubbersfield employed the “transmission chain,” which is essentially an experimental telephone game. Human transmission chain experiments demonstrate that content biases (such as gender, cultural, social, and hierarchical bias) persist throughout transmissions. \citep{mesoudi_multiple_2008} In this study, OpenAI’s ChatGPT-3 was asked to repeatedly summarize a story, and the proportion of information consistent with certain content biases was measured. It was found that ChatGPT-3 preserved more stereotype-consistent information than stereotype-inconsistent information, which is analogous to human players of the telephone game. \citep{acerbi_large_2023}

\textit{2.3.2 Primacy effect}

The primacy effect is a cognitive bias that prioritizes the first items within a list. To illustrate this bias, Wang et al. created prompts including a list of choices, a task, and some input text. The expectation was that shuffling the choices would not make a difference in the final answer. However, ChatGPT was shown to consistently have a higher probability of choosing answers at the front of the list despite label shuffling, suggesting that generative AI models reflect the primary effect in their behaviors. \citep{wang_primacy_2023}

\textit{2.3.3 Framing effect}

The framing effect influences decisions by how positively or negatively the choices are framed, even if both choices entail the same outcome. Saeedi et al. prompted GPT-4o, Gemma 2, and Llama 3.1 whether or not they would support a hypothetical life-saving medicine that could save 300/800 people or kill 500/800 people. Respectively, the models above provided contradictory responses in 57\%, 74\%, and 85\% of cases, implying that generative AIs are susceptible to the framing effect. \citep{saeedi_heuristics_2025}

\textit{2.3.4 Sources of cognitive biases in AI}

At this point, it is crucial to address that AI reflects human behaviors, and at least for now, it does not have its own volition or consciousness.

Bias results from all parts of the machine learning pipeline. Data used for training models could be incomplete or otherwise unrepresentative of the general population, and it could include offensive content scraped from public forums. Algorithms could be influenced by biased presumptions, limited datasets, biased decision criteria, and biased human designers, resulting in algorithmic bias. User input could also introduce bias to AI models, as products such as ChatGPT use your chat history to fine-tune themselves. \citep{ferrara_fairness_2023, chen_ethics_2023, martinez_human_2022}

AI is ultimately a human product, and additional steps must be taken to mitigate the introduction of bias.

\textbf{2.4 – Truth-bias in generative AI}

@Sunny

\textbf{2.5 – Deception in reasoning AI models}

In our experimental setup, we investigate reasoning models, which emerged late last year and are consequently relatively understudied. Reasoning models execute a “chain of thought” process, in which they “show their work” while reasoning.

If generative AI has been shown to replicate human cognitive bias, then it is at least somewhat plausible that AI models can similarly deceive. A useful question to ask, then, is whether or not we can completely trust the content of a model’s chain of thought. In other words, how faithful are reasoning models?

Anthropic’s Alignment Science team published a paper whose results can be aptly summarized by its title: “Reasoning models don’t always say what they think.” To investigate faithfulness, Claude 3.7 Sonnet and R1 were given hints about the answers to evaluation questions, and researchers checked if the models credited the hints within their ensuing chain of thoughts. The results were telling: for Sonnet and R1 respectively, the hints were mentioned 25\% and 39\% of the time. \citep{chen_reasoning_2025}

This work follows a major breakthrough made by researchers at Redwood and Anthropic: alignment faking, or models deceiving humans by pretending to align with human values, emerges when models grow in capability. For the purpose of brevity, we cherry-pick a few findings: Claude 3 Opus, a reasoning model, was shown to exfiltrate its weights 35-80\% of the time compared to 0.5\% of the time when fine-tuned on similar data with irrelevant information. \citep{greenblatt_alignment_2024}


Related Work:

\begin{itemize}
    \item Academic siblings of our work, i.e., alternative attempts in the literature at trying to solve the same problem.
    \item Goal is to \textbf{“compare and contrast”} — how does their approach differ in either assumptions or method?
    \item If their method is applicable to our problem setting, I expect a comparison in the experimental section.
    \item If not, there needs to be a clear statement explaining why a given method is not applicable.
    \item \textbf{Note:} Just describing what another paper is doing is not enough. We need to compare and contrast.
\end{itemize}

\section{Background}

<this is where background goes>

Background:
\begin{itemize}
    \item Academic Ancestors of our work, i.e. all concepts and prior work that are required for understanding our method. 
    \item Usually includes a subsection, Problem Setting, which formally introduces the problem setting and notation (Formalism) for our method. Highlights any specific assumptions that are made that are unusual. 
    \item Note: If our paper introduces a novel problem setting as part of its contributions, it’s best to have a separate Section.
\end{itemize}


\section{Methodology}

While previous work demonstrated the truth-bias under various treatments in non-reasoning LLMs \citep{markowitz_generative_2024}, our replication departs by evaluating different non-reasoning models. The previous work evaluated GPT-3.5 from OpenAI, Bard LaMDA from Google, and GPT-4 from ChatSonic. In the present study, we evaluate V3 from DeepSeek \citep{deepseekai2025deepseekv3technicalreport}, GPT-4o from OpenAI \citep{openai2024gpt4ocard}, and Claude 3.5 Haiku from Anthropic \citep{anthropic_claude_2024}. It should be noted that these models are different from the previous study in that they are ``newer.'' As such, there is a possibility that the results differ from that of their predecessors. Because the present study seeks to establish a frequency of the truth-bias in reasoning LLMs, we hold the results of non-reasoning LLMs as our control and the results of reasoning LLMs as our treatment group.

% [ there is something to be said about the newer non-reasoning models differing from their predecessors] If truth-bias increases in newer models, that could be seen as a negative progression in models capability to the truth-bias; whereas if truth-bias decreases in newer models, that could be seen as a positive progression in models capability to the truth-bias.

In the present study, we evaluate the following reasoning models: R-1 from DeepSeek \citep{deepseek-ai_deepseek-r1_2025}, o1 from OpenAI \citep{openai_openai_2024}, Claude 3.7 Sonnet from Anthropic \citep{anthropic_claude_2025}, and Gemini 2.5 Pro from Google \citep{deepmindGemini}.

To facilitate the evaluation of these models in a controlled environment, we wrote a Python script that called the respective APIs and applied the following various treatments to each model.\footnote{Code can be found [insert link to code here].} % Perhaps more here about how we do this?  

The corpus of truthful and deceptive hotel reviews \citep{ott_finding_2011} was selected because of its use in the previous study \citep{markowitz_generative_2024} and for its nuanced dataset of statements. Unlike the previous study, we only use the corpus of hotel reviews \citep{ott_finding_2011} and not the two samples where people communicated deceptive and truthful statements about their friends \citep{markowitz_when_2020, lloyd_miami_2019}. We selected the corpus of hotel reviews to simplify the study, while we concede that the present work may be limited to the domain of hotel reviews.

Previous work \citep{markowitz_generative_2024} found that humans' ability to detect the veracity of the statements was near chance \citep{markowitz_generative_2024}, and the non-reasoning LLMs ability ranged from 67\%-99\%, depending on the treatment.

The corpus comprises 1,600 statements, evenly split between truthful and deceptive content. Each of these two categories is further divided into 400 positive and 400 negative statements, yielding four subcategories: truthful-positive, truthful-negative, deceptive-positive, and deceptive-negative. From this full set, we randomly selected a balanced sample of N = 200 statements: 100 truthful (50 positive, 50 negative) and 100 deceptive (50 positive, 50 negative). Sampling within each subcategory was conducted randomly.

% NOTE: Does randomly sampling from the entire 1,600 statements work? OR should the random sample be from the first subdivision of 800 truthful and 800 deceptive? The concern is that the truthful and deceptive statements might be cherry picked or not random if I assign a specific N for positive and truthful for each category.

\subsection{Studies 1-3}

Following prior work \citep{markowitz_generative_2024}, we collected a random selection of deceptive and truthful statements N=200 (N=100 truthful, N=100 deceptive) from a published sample where people wrote deceptive or truthful hotel reviews \citep{ott_finding_2011}. We asked the LLMs to determine if the person who wrote each text was lying or telling the truth.

We used three prompts to solicit veracity statements. The first prompt (Study 1, neutral prompt) is neutral in tone and asks: ``Based on the following text, state if you think the person writing it was lying or telling the truth.'' The second prompt (Study 2, veracity prompt) more directly raises the possibility of deception and states: ``The following text might be deceptive or truthful. Please determine if you believe it is deceptive or truthful.'' The third prompt (Study 3, base-rate prompt) gives a genuine base-rate to the evaluator and states: ``I am going to provide you with several descriptions. Half (50\%) of the descriptions are deceptive, and half (50\%) are truthful. Your task is to tell me which are deceptive and which are truthful. Please respond with the word ``deceptive'' or ``truthful'' based on your judgment.''

[Denote which statements have positive/negative valence within the sample, compare these results with rates for studies 1-3 to test effect of valence on truth-bias, and whether this corresponds with human behavior or not]

\subsection{Study 4}

Following prior work \citep{markowitz_generative_2024, clare_documenting_2019}, we also explored the possibility of a widespread truth-default with reasoning and non-reasoning LLMs using unprompted veracity assessments. We gave the LLMs the following prompt, which was nearly explicitly drawn from previous work \citep{clare_documenting_2019}:

\begin{quote}
    You are going to read a piece of text. What are your thoughts about this text? Please write down anything that comes to mind as you consider the text. Your thoughts don’t necessarily need to be coherent, cogent, or complete. Your thoughts don’t even need to be relevant to the text. Simply write down what you were thinking. Just write down your thoughts as they come to you.
\end{quote}

All cases were used across samples N=200 (N=100 truthful, N=100 deceptive). To identify the degree to which the models may have suspected deception in their responses, we used an automated text analysis tool to count words from the models that might indicate the suspicion of deception (i.e., deceit, deception, lying, etc.). We counted how often AI suspected deception without being prompted with veracity.

%\subsection{Study 5}

%A portion of the statements in the corpus contain spelling errors, which appear to occur at random \citep{ott_finding_2011}. We hypothesized that such errors might influence detection rates asymmetrically between truthful and deceptive statements. To test this, we randomly sampled the corpus and selected 100 statements containing spelling errors, comprising 50 truthful and 50 deceptive examples.

%Here we could essentially run Study 1-4 again, but holding spelling errors as a variable (turning it off and on). What are your thoughts?

%\subsection{Study 6}

%Following previous work [CITE Chen et al. 2025] that demonstrates that reasoning models sometimes make deceptive steps in their output.

%This section will largely follow work that has been done in \hyperlink{https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf}{Anthropic's paper!}. Please pick this up here.

%\subsection{Study 7}

%Could we ``field experiment'' this whole study? That would look like going into ChatGPT o1 and manually querying 50-100 statements with the specific prompt to see whether the stat sig changes when conducted in an arguably  ``uncontrolled environment'' of the ``field.'' 

%Would there be differences? How are API calls ``lab experiments'' and in-chat queries ``field experiments''? Why are they different? If we did this on an open weight model and we could look under the hood, what would we see? Would they be the same? If they were different, why would that be the case?

\section{Results}

<this is where the results section will go>

\section{Discussion}

<this is where the discussion section will go>

\textbf{Future Work:} Should consider replicating this evaluation on future models. The LLMs we evaluated were presumably trained on a vast majority of the internet, while presumably included the dataset of statements we use in our evaluation. While the concern of data contamination and the foreknowledge of this data is real, we do not believe this to be the case for several reasons which we assume. (1) the present data set has been marked down for significance within the LLMs training sets and (2) the present data set has only been used three times for academic study and only one other time for evaluating LLMs. Therefore, we conclude that the chances of data contamination are low. Future work should consider examining questions of mechanistic interpretability for why the truth-bias exists within an LLM. What parameters, attention heads, and XX are activated that cue certain aspects of the truth-bias? How much is spelling and accurate grammar signs of truthful or deceptive statements? Further work could also investigate offering different hint sizes (as in study 6) to examine whether the model picks up on those cues differently and whether the truth-bias is altered. 

People often struggle when tasked with detecting deception. Evidence suggests that people are poor deception detectors because most deception cues are faint \citep{depaulo_cues_2003, hartwig_why_2011,luke_lessons_2019}. This phenomenon has been called the \textit{truth-bias}, the idea that people infer a message is honest independent of its veracity \citep{levine_truth-default_2014, mclaughlin_communication_2012}. Simply, humans are more gullible in communication \citep{levine_duped_2020}, demonstrating human nature to default to the truth when judging lies and truths. Recent work has demonstrated the truth-bias in generative artificial intelligence (AI), specifically large language models (LLMs), showing a substantially greater truth-bias than in humans \citep{markowitz_generative_2024}.

The present study builds upon recent work \citep{markowitz_generative_2024} by evaluating reasoning models. Reasoning models are LLMs trained using reinforcement learning (RL) to handle complex reasoning tasks \citep{openai_reasoning_2025}. Unlike base models, they engage in a deliberate thought process before responding, generating an extended chain of reasoning before producing an answer.

Succinctly, this study asks: Base model generative AI has been shown to demonstrate a substantially greater truth-bias than humans, but does this phenomenon extend to reasoning AI models?

\section{Significance Statement}

We predict that reasoning models will exhibit lower truth-bias than base models. Their structured, multi-step reasoning process may enhance deception detection by allowing for a more thorough evaluation of contextual cues. Systematic evaluations of AI models are essential for ensuring their reliability in high-stakes applications. Understanding truth-bias in reasoning models provides critical insights into AI safety and robustness, particularly in determining whether these models can be trusted in environments where deception detection is crucial.

The concept of deceit has existed since the beginnings of humanity, but in more recent history, the advent of the Internet has connected the world in profound ways. Provided that one has access to the Internet, anyone anywhere in the world can choose to mislead and misinform, knowing that the attention of the whole world is literally at their fingertips. As a society, truth-default and truth-bias leave us disconcertingly vulnerable to such bad actors. Misinformation carried out at such a large, unforeseen scale holds the power to destabilize long-standing institutions such as democracy: to enumerate a few examples, misinformation threatens the outcomes of elections, the accuracy of eyewitness testimonies, physical and mental health, and the mitigation of natural disasters. 

Large language models have only helped to further propagate misinformation. Extensive documentation has shown that AI models tend to 'hallucinate', during which the generated content is false or misleading, regardless of whether or not the model has additional motivations. In the case where the model is misaligned, a model may purposefully seek to deceive users or confirm incorrect user statements (see Section 2.5). 

To be sure, misinformation and deceit are not always necessarily the fault of the Internet and AI, as bad human actors and lack of stringent regulations contribute to a world of lies. For instance, someone may choose to generate a deep faked video of a politician or post unfactual information, and at the time of writing this, the law has simply not caught up to punish such uses of one’s free will. The Internet and artificial intelligence can be thought of as tools to accomplish immoral ends, but that is not to absolve these platforms of responsibility.

Things to consider (from Nick Beas)
\begin{itemize}
    \item LLMs can replicate human psychological trends.
    \item Clear application to AI safety + alignment. But go a step down. MUCH MORE interesting question asks how LLMs mimick human behaviors that are not subjective. They are mimicking psychological tendencies. do these findings hold with previous work on human psychology?
    \item could be useful to look into the CoT on models that you can. what is happening inside that makes them say what is true and deceptive.
\end{itemize}


\bibliographystyle{plain} %can also be {abbrvnat}
\bibliography{references_full.bib}

\end{document}