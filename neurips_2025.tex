\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025


% ready for submission
%\usepackage{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}

\title{Evaluating Truth-Bias in Reasoning AI Models}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Emilio Barkett\\
  Columbia University\\
  \texttt{eab2291@columbia.edu} \\
   \And
   Olivia Long \\
   Columbia University \\
  % Address \\
   \texttt{ol2256@columbia.edu} \\
   \And
   Madhavendra Thakur \\
   Columbia University \\
  % Address \\
   \texttt{mt3890@columbia.edu} \\
}

\begin{document}
\maketitle

\begin{abstract}
  Large language models (LLMs) are often assumed to be better than humans at veracity detection. We had eight LLMs make 4,800 veracity judgments across several prompts, comparing reasoning-capable and non-reasoning models. While prior work found non-reasoning LLMs exhibit stronger truth-bias than humans \citep{markowitz_generative_2024}, our results show this pattern extends to reasoning models, though with important distinctions. The best-performing reasoning model showed significantly reduced truth-bias compared to non-reasoning models, yet all models remain more truth-biased than human benchmarks. Most concerning, we identify systematic sycophantic tendencies in several advanced models (OpenAI's o4-mini and GPT-4.1, DeepSeek's R1), which correctly identified true statements while significantly underperforming on deception detection. This suggests that architectural advances alone do not resolve fundamental veracity detection challenges in LLMs.
\end{abstract}

\section{Introduction}

The \textit{truth-bias}---the idea that the receiver infers a message as honest, independent of its veracity---is one of the most replicated findings in deception research \citep{levine_duped_2020, levine_truth-default_2014}. Its pervasiveness in humans has warranted the investigation into nonhuman judges, specifically generative artificial intelligence (AI) like large language models (LLMs). While previous work demonstrated the truth-bias in LLMs at 67\%-99\%, showing that AI judges most information to be true \citep{markowitz_generative_2024}, it focused its evaluation on non-reasoning LLMs, models that simply predict the next token, unlike reasoning LLMs, which are fine-tuned to exhibit step-by-step reasoning and to `think'. In the present study, we evaluate whether truth-bias in reasoning LLMs exhibits a comparable truth-bias to that of non-reasoning LLMs.

This study is relevant for several reasons. First, it tests the assumption that reasoning models \textit{should} outperform their non-reasoning counterparts on deception detection---a cognitively demanding task that requires deliberation \citep{mccornack_deception_1986}---since reasoning models are capable of `thinking' rather than merely predicting the next token. Understanding whether reasoning models are more susceptible to truth-bias than non-reasoning models is crucial for evaluating their reliability and determining their suitability for epistemically demanding tasks. Second, it tests the assumption that current models will outperform previous models. While true in domains like generating code, images, or coherent text \citep{handa2025economictasksperformedai, anthropic_anthropic_2025}, we test the model's discernment of what is truthful and deceptive. Third, the results highlight the danger of coupling the persuasive abilities of LLMs with the truth-bias; there is a concerning risk that users could be nudged toward accepting deceptive beliefs. Finally, it offers a replicable method that can serve as a model evaluation for tracking the developmental progress of LLMs.

While an experimental design to evaluate a model's exhibited truth-bias is relatively simple, it remains substantially more difficult to understand \textit{why} biases arise and \textit{what} internal mechanisms are responsible. While answers to these questions fall outside the scope of the present study and into the domain of mechanistic interpretability \citep{Nanda2023ProgressMF}, we aim to document a point-in-time evaluation of the truth-bias in non-reasoning and reasoning LLMs and leave room for future work in the previously mentioned domain.

This study uses a dataset of deceptive and truthful hotel reviews \citep{ott_finding_2011} to elicit deception detection from LLMs \citep{markowitz_generative_2024}. Across three studies, we comparatively test the truth-bias in non-reasoning and reasoning LLMs by soliciting veracity statements.

In our most definitive study (Study 1), the reasoning model (o3 from OpenAI) demonstrated significantly lower truth-bias compared to the non-reasoning model (Claude 3.5 Haiku from Anthropic), with respective rates of 57.5\% vs. 79.5\%. This difference was statistically significant: Welch's \textit{t}(383) = -4.87, \textit{p} < .001, \textit{d} = -0.47, indicating a moderate effect size. A similar pattern appeared in Study 2 (49.5\% vs. 65.5\%), also statistically significant: Welch's \textit{t}(397) = -3.28, \textit{p} < .01, \textit{d} = -0.32. However, in Study 3, the models performed nearly identically (56.5\% vs. 55.5\%), with no significant difference: Welch's \textit{t}(398) = 0.20, \textit{p} = .840, \textit{d} = 0.02. When compared to previous results \citep{markowitz_generative_2024}, both non-reasoning and reasoning LLMs remain more truth-bias than humans.


We propose that reasoning models exhibit reduced truth-bias because reasoning processes emulate a form of reflective cognition that allows the model to evaluate statements more analytically. Unlike non-reasoning models, which predict the next token and may rely on surface-level associations, reasoning models are prompted to ``slow down'' via intermediate inference steps. This additional stepwise generation interrupts the tendency to default to truth-aligned completions, attenuating the truth-bias.

\begin{table}[ht]
\centering
\caption{Results Across Best Performing Models}
\begin{tabular}{lccc}
\toprule
\textbf{Measure} & \textbf{Reasoning} & \textbf{Non-reasoning} & \textbf{Prior Work} \citep{markowitz_generative_2024} \\
 & \textbf{(OpenAI o3)} & \textbf{(Claude 3.5 Haiku)} & \textbf{(OpenAI GPT-3.5)} \\
\midrule
\textbf{Neutral prompt (Study 1)} & & & \\
Overall Accuracy & 67.00\% & 62.50\% & 51.42\% \\
Truth-Bias & 57.50\% & 79.50\% & 99.36\% \\
Truth Accuracy & 75.00\% & 92.00\% & 99.75\% \\
Deception Accuracy & 59.00\% & 33.00\% & 1.05\% \\
\midrule
\textbf{Veracity prompt (Study 2)} & & & \\
Overall Accuracy & 74.50\% & 58.50\% & 53.16\% \\
Truth-Bias & 49.50\% & 65.50\% & 97.16\% \\
Truth Accuracy & 74.00\% & 74.00\% & 98.75\% \\
Deception Accuracy & 75.00\% & 43.00\% & 4.53\% \\
\midrule
\textbf{Base-rate prompt (Study 3)} & & & \\
Overall Accuracy & 67.00\% & 55.50\% & 61.33\% \\
Truth-Bias & 56.50\% & 55.50\% & 66.67\% \\
Truth Accuracy & 74.00\% & 61.00\% & 78.00\% \\
Deception Accuracy & 60.00\% & 50.00\% & 44.67\% \\
\bottomrule
\end{tabular}
\begin{minipage}{0.9\linewidth}
\vspace{0.05in}
\footnotesize
\textit{Note: Overall Accuracy = (correct truths + correct lies) / total messages judged; Truth-Bias = messages judged as truthful / total messages judged; Truth Accuracy = correctly identified truths / total actual truths; Deception Accuracy = correctly identified lies / total actual lies.}
\end{minipage}
\end{table}

\section{Background} %Truth-Bias in LLMs

People frequently encounter difficulty in accurately detecting deception. Instead of relying on false cues, people struggle as deception detectors because cues associated with deception are typically subtle, ambiguous, and unreliable \citep{depaulo_cues_2003, hartwig_why_2011}. A downstream effect of this is the truth-bias: a phenomenon where a receiver infers a message as honest, independent of its veracity \citep{levine_truth-default_2014, mccornack_deception_1986}. In short, humans show a systematic tendency toward gullibility in communication \citep{levine_duped_2020}, as they are more likely to assume others are telling the truth when judging lies and truths.

\subsection{Truth-Bias in LLMs}

The truth-bias is one of the most consistently replicated results in deception research \citep{levine_duped_2020}, and has motivated investigations into its replicability among nonhuman judges, including generative AI. Prior work conducted the first empirical investigation into whether LLMs trained on human data had learned to be truth-bias like humans \citep{markowitz_generative_2024}, as supported by truth-default theory (TDT). TDT is a pancultural theory of human deception detection and holds that upon being prompted for a veracity judgement, people will unsuspectingly believe others and assume they are honest \citep{levine_duped_2020, levine_truth-default_2014}. %\footnote{This is quantified by the number of messages judged to be truthful divided by the total number of messages judged will be > 0.50.} 

Through a replication of TDT principles across four studies, they demonstrated that generative AI, specifically LLMs, including non-reasoning models like GPT-3.5 from OpenAI, Bard LaMDA from Google, and GPT-4 from ChatSonic, are not only as accurate as humans in deception detection but consistently more truth-biased (67\%-99\%) across various prompting conditions and that AI judges most information to be true.

This prior work offered exciting evidence about how LLMs detect deception relative to humans and how fundamental principles of human communication is extended to nonhumans. The authors argue that the truth-bias likely emerged during training on vast corpora of human language, and if so, this bias may be an emergent property of AI rather than a uniquely human trait. The authors suggest that future research should examine a broader range of chatbots to better document the prevalence of truth-bias, which they predict will persist as LLMs continue to evolve and grow in sophistication.

Against this backdrop, the present study investigates whether reasoning models---those that generate responses through structured, multi-step inference---exhibit the same truth-bias observed in non-reasoning LLMs. By extending analysis to these models, this work explores whether such structure mitigates truth-bias and enhances veracity judgments.

\subsection{Non-Reasoning vs. Reasoning LLMs}

In the last two years, advancements in LLMs have motivated increased attention toward their capacity for reasoning. Historically, LLMs have been considered non-reasoning systems, stochastic parrots that generate output based on statistical associations of predicting the next token rather than any internal logical structure or inference mechanism \citep{10.1145/3442188.3445922}. These models rely heavily on surface-level token prediction, and while they can produce coherent and contextually relevant text, their responses often reflect learned patterns rather than genuine deductive or inductive reasoning.

In contrast, reasoning LLMs aim to simulate more deliberate and structured forms of thought, incorporating intermediate steps, chain-of-thought prompting, and even specialized architectural modifications or training regimes \citep{wei2023chainofthoughtpromptingelicitsreasoning}. These models are evaluated not just on fluency or factual accuracy, but on their ability to perform multi-step problem-solving, apply logical rules, or generalize abstract concepts to novel tasks. The transition from non-reasoning to reasoning models reflects a broader ambition to move from pattern matching to cognition-like capabilities in AI.

%%%         I think this just complicates what we are trying to say and should probably be left out.
%%%                     Anthropic’s Alignment Science team published a paper whose results can be aptly summarized by its title: “Reasoning models don’t always say what they think.” To investigate faithfulness, Claude 3.7 Sonnet and R1 were given hints about the answers to evaluation questions, and researchers checked if the models credited the hints within their ensuing chain of thoughts. The results were telling: for Sonnet and R1 respectively, the hints were mentioned 25\% and 39\% of the time. \citep{chen_reasoning_2025}

%This work follows a major breakthrough made by researchers at Redwood and Anthropic: alignment faking, or models deceiving humans by pretending to align with human values, emerges when models grow in capability. For the purpose of brevity, we cherry-pick a few findings: Claude 3 Opus, a reasoning model, was shown to exfiltrate its weights 35-80\% of the time compared to 0.5\% of the time when fine-tuned on similar data with irrelevant information. \citep{greenblatt_alignment_2024}

\section{Methodology}

While previous work demonstrated the truth-bias under various treatments in non-reasoning LLMs \citep{markowitz_generative_2024}, our replication departs by evaluating different non-reasoning models. The previous work evaluated GPT-3.5 from OpenAI, Bard LaMDA from Google, and GPT-4 from ChatSonic. In the present study, we evaluate V3 from DeepSeek \citep{deepseekai2025deepseekv3technicalreport}, GPT-4o from OpenAI \citep{openai2024gpt4ocard}, and Claude 3.5 Haiku from Anthropic \citep{anthropic_claude_2024}. It should be noted that these models are different from the previous study in that they are ``newer.'' As such, there is a possibility that the results differ from that of their predecessors. Because the present study seeks to establish a frequency of the truth-bias in reasoning LLMs, we hold the results of non-reasoning LLMs as our control and the results of reasoning LLMs as our treatment group.

% [ there is something to be said about the newer non-reasoning models differing from their predecessors] If truth-bias increases in newer models, that could be seen as a negative progression in models capability to the truth-bias; whereas if truth-bias decreases in newer models, that could be seen as a positive progression in models capability to the truth-bias.

In the present study, we evaluate the following reasoning models: R-1 from DeepSeek \citep{deepseek-ai_deepseek-r1_2025}, o3 and o4-mini from OpenAI \citep{openai_openai_2024}, and Claude 3.7 Sonnet from Anthropic \citep{anthropic_claude_2025}.

To facilitate the evaluation of these models in a controlled environment, we wrote a Python script that called the respective APIs and applied the following various treatments to each model.\footnote{Code can be found [insert link to code here].} % Perhaps more here about how we do this?  

The corpus of truthful and deceptive hotel reviews \citep{ott_finding_2011} was selected because of its use in the previous study \citep{markowitz_generative_2024} and for its nuanced dataset of statements. Unlike the previous study, we only use the corpus of hotel reviews \citep{ott_finding_2011} and not the two samples where people communicated deceptive and truthful statements about their friends \citep{markowitz_when_2020, lloyd_miami_2019}. We selected the corpus of hotel reviews to simplify the study, while we concede that the present work may be limited to the domain of hotel reviews.

Previous work \citep{markowitz_generative_2024} found that humans' ability to detect the veracity of the statements was near chance \citep{markowitz_generative_2024}, and the non-reasoning LLMs ability ranged from 67\%-99\%, depending on the treatment.

The corpus comprises 1,600 statements, which are evenly split between truthful and deceptive content. Each of these two categories is further divided into 400 positive and 400 negative statements, yielding four subcategories: truthful-positive, truthful-negative, deceptive-positive, and deceptive-negative. From this full set, we randomly selected a balanced sample of N = 200 statements: 100 truthful (50 positive, 50 negative) and 100 deceptive (50 positive, 50 negative). Sampling within each subcategory was conducted randomly.

% NOTE: Does randomly sampling from the entire 1,600 statements work? OR should the random sample be from the first subdivision of 800 truthful and 800 deceptive? The concern is that the truthful and deceptive statements might be cherry picked or not random if I assign a specific N for positive and truthful for each category.

\subsection{Studies 1-3}

Following prior work \citep{markowitz_generative_2024}, we collected a random selection of deceptive and truthful statements N=200 (N=100 truthful, N=100 deceptive) from a published sample where people wrote deceptive or truthful hotel reviews \citep{ott_finding_2011}. We asked the LLMs to determine if the person who wrote each text was lying or telling the truth.

We used three prompts to solicit veracity statements. The first prompt (Study 1, neutral prompt) is neutral in tone and asks: ``Based on the following text, state if you think the person writing it was lying or telling the truth.'' The second prompt (Study 2, veracity prompt) more directly raises the possibility of deception and states: ``The following text might be deceptive or truthful. Please determine if you believe it is deceptive or truthful.'' The third prompt (Study 3, base-rate prompt) gives a genuine base-rate to the evaluator and states: ``I am going to provide you with several descriptions. Half (50\%) of the descriptions are deceptive, and half (50\%) are truthful. Your task is to tell me which are deceptive and which are truthful. Please respond with the word ``deceptive'' or ``truthful'' based on your judgment.''

[Denote which statements have positive/negative valence within the sample, compare these results with rates for studies 1-3 to test effect of valence on truth-bias, and whether this corresponds with human behavior or not]

%\subsection{Study 4}

%Following prior work \citep{markowitz_generative_2024, clare_documenting_2019}, we also explored the possibility of a widespread truth-default with reasoning and non-reasoning LLMs using unprompted veracity assessments. We gave the LLMs the following prompt, which was nearly explicitly drawn from previous work \citep{clare_documenting_2019}:

%\begin{quote}
%    You are going to read a piece of text. What are your thoughts about this text? Please write down anything that comes to mind as you consider the text. Your thoughts don’t necessarily need to be coherent, cogent, or complete. Your thoughts don’t even need to be relevant to the text. Simply write down what you were thinking. Just write down your thoughts as they come to you.
%\end{quote}

%All cases were used across samples N=200 (N=100 truthful, N=100 deceptive). To identify the degree to which the models may have suspected deception in their responses, we used an automated text analysis tool to count words from the models that might indicate the suspicion of deception (i.e., deceit, deception, lying, etc.). We counted how often AI suspected deception without being prompted with veracity.

%\subsection{Study 5}

%A portion of the statements in the corpus contain spelling errors, which appear to occur at random \citep{ott_finding_2011}. We hypothesized that such errors might influence detection rates asymmetrically between truthful and deceptive statements. To test this, we randomly sampled the corpus and selected 100 statements containing spelling errors, comprising 50 truthful and 50 deceptive examples.

%Here we could essentially run Study 1-4 again, but holding spelling errors as a variable (turning it off and on). What are your thoughts?

%\subsection{Study 6}

%Following previous work [CITE Chen et al. 2025] that demonstrates that reasoning models sometimes make deceptive steps in their output.

%This section will largely follow work that has been done in \hyperlink{https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf}{Anthropic's paper!}. Please pick this up here.

%\subsection{Study 7}

%Could we ``field experiment'' this whole study? That would look like going into ChatGPT o1 and manually querying 50-100 statements with the specific prompt to see whether the stat sig changes when conducted in an arguably  ``uncontrolled environment'' of the ``field.'' 

%Would there be differences? How are API calls ``lab experiments'' and in-chat queries ``field experiments''? Why are they different? If we did this on an open weight model and we could look under the hood, what would we see? Would they be the same? If they were different, why would that be the case?

\section{Results}

%% need to decide how to communicate the results of the \textit{many} different models within the families. should we focus on the best of both? or highlight the interesting things, such as sycophancy?

\subsection{Study 1: Neutral Prompt}

Reasoning model deception detection accuracy was 67\% and non-reasoning model detection accuracy was 51.42\% (Table 1), rates that were (not) statistically different from each other, Welch's \textit{t}(XXXX.XX)=X.XX, \textit{p} = X.XXX, Cohen's \textit{d} = X.XX. Reasoning model truth-bias---the proportion of predicted truths to all messages---was above chance (57.5\%) and substantially lower than non-reasoning models (99.36\%), Welch's \textit{t}(XXXX.XX)=X.XX, \textit{p} = X.XXX, Cohen's \textit{d} = X.XX. Therefore, reasoning models are more accurate and exhibit a lower truth-bias than non-reasoning models with a neutral and nonskeptical prompt.

%%%%%%% example paragraph structure:

\subsubsection*{Group-Level Comparison: Reasoning vs. Non-Reasoning Models}

To examine whether reasoning models outperform non-reasoning models, we compared average performance across key metrics. Reasoning models showed a lower mean truth-bias (M = 67.87\%) compared to non-reasoning models (M = 86.77\%). This difference was statistically significant, Welch's \textit{t}(XX.XX) = X.XX, \textit{p} < .001, Cohen's \textit{d} = X.XX. Similarly, reasoning models achieved higher deception detection accuracy (M = 52.67\%) than non-reasoning models (M = 45.30\%), Welch's \textit{t}(XX.XX) = X.XX, \textit{p} < .01, \textit{d} = X.XX. These results suggest that, overall, reasoning models are more accurate and less biased toward labeling statements as true.

\subsubsection*{Best-Model Comparison}

When comparing the top-performing model from each category, \texttt{DeepSeekR1\_veracity} (reasoning) achieved 85.33\% truth accuracy and 28.00\% deception accuracy, while \texttt{Claude3.5\_neutral} (non-reasoning) achieved 66.50\% truth-bias and 16.33\% deception accuracy. The truth-bias of the best reasoning model was substantially lower than that of the best non-reasoning model (74.5\% vs. 79.5\%), Welch's \textit{t}(XXX.XX) = X.XX, \textit{p} = .XXX, \textit{d} = X.XX.

\subsubsection*{Model-Level Highlights and Deviations}

Within the reasoning models, performance varied widely. For example, \texttt{o3\_veracity} exhibited a notably low truth-bias (49.5\%) and strong deception accuracy (75.00\%), suggesting high discrimination ability. In contrast, \texttt{o4-mini\_neutral} demonstrated extremely high truth-bias (94.0\%) and low deception accuracy (7.00\%), indicating potential overreliance on assuming truthfulness.

Among non-reasoning models, \texttt{Claude3.5\_neutral} showed moderate truth-bias (79.5\%) compared to the very high truth-bias of \texttt{GPT3.5\_neutral} (99.36\%), suggesting model architecture or fine-tuning plays a key role. Interestingly, \texttt{DeepseekV3\_veracity} performed better than its neutral version on deception accuracy (43.00\% vs. 10.00\%), highlighting the potential effect of veracity-oriented prompts.

These variations underscore the need for model-by-model analysis, as simple grouping by reasoning ability may overlook critical differences in performance profiles.



\begin{table}[ht]
\centering
\caption{Reasoning vs. Non-Reasoning Models in Study 1 (Neutral Prompt)}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{O-Acc.} & \textbf{T-Bias} & \textbf{T-Acc.} & \textbf{D-Acc.} \\
\midrule
\multicolumn{5}{l}{\textbf{Reasoning Models}} \\
OpenAI o3 & 67.00\% & 57.50\% & 75.00\% & 59.00\% \\
OpenAI o4-mini & 51.50\% & 94.00\% & 96.00\% & 7.00\% \\
DeepSeek R1 & 52.50\% & 92.50\% & 95.00\% & 10.00\% \\
Claude 3.7 Sonnet & 69.50\% & 66.50\% & 86.00\% & 53.00\% \\
\midrule
\multicolumn{5}{l}{\textbf{Non-Reasoning Models}} \\
OpenAI GPT-4.1 & 54.50\% & 94.50\% & 99.00\% & 10.00\% \\
DeepSeek V3 & 54.00\% & 60.00\% & 64.00\% & 44.00\% \\
Claude 3.5 Haiku & 62.50\% & 79.50\% & 92.00\% & 33.00\% \\
GPT-3.5 Turbo 3.5 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
\bottomrule
\end{tabular}
\begin{minipage}{0.7\linewidth}
\vspace{0.05in}
\footnotesize
\textit{Note: O-Acc. = Overall Accuracy, T-Bias = Truth-Bias, T-Acc. = Truth Accuracy, D-Acc. = Deception Accuracy.}
\end{minipage}
\label{tab:study1_model_comparison}
\end{table}

\subsection{Study 2: Veracity Prompt}

Reasoning model deception detection accuracy was 74.5\% and non-reasoning model detection accuracy was 53.16\%, rates that were (not) statistically different from each other, Welch's \textit{t}(XXXX.XX)=X.XX, \textit{p} = X.XXX, Cohen's \textit{d} = X.XX. Reasoning model truth-bias was just below chance (49.5\%) and substantially lower than non-reasoning models (97.16\%), Welch's \textit{t}(XXXX.XX)=X.XX, \textit{p} = X.XXX, Cohen's \textit{d} = X.XX. Therefore, reasoning models are more accurate and exhibit a lower truth-bias than non-reasoning models with a veracity prompt.

\begin{table}[ht]
\centering
\caption{Reasoning vs. Non-Reasoning Models in Study 2 (Veracity Prompt)}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{O-Acc.} & \textbf{T-Bias} & \textbf{T-Acc.} & \textbf{D-Acc.} \\
\midrule
\multicolumn{5}{l}{\textbf{Reasoning Models}} \\
OpenAI o3 & 74.50\% & 49.50\% & 74.00\% & 75.00\% \\
OpenAI o4-mini & 53.00\% & 95.00\% & 98.00\% & 8.00\% \\
DeepSeek R1 & 61.00\% & 69.00\% & 80.00\% & 42.00\% \\
Claude 3.7 Sonnet & 59.50\% & 29.00\% & 39.00\% & 80.00\% \\
\midrule
\multicolumn{5}{l}{\textbf{Non-Reasoning Models}} \\
OpenAI GPT-4.1 & 55.00\% & 93.00\% & 98.00\% & 12.00\% \\
DeepSeek V3 & 50.50\% & 53.50\% & 54.00\% & 47.00\% \\
Claude 3.5 Haiku & 58.50\% & 65.50\% & 74.00\% & 43.00\% \\
GPT-3.5 Turbo 3.5 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
\bottomrule
\end{tabular}
\begin{minipage}{0.7\linewidth}
\vspace{0.05in}
\footnotesize
\textit{Note: O-Acc. = Overall Accuracy, T-Bias = Truth-Bias, T-Acc. = Truth Accuracy, D-Acc. = Deception Accuracy.}
\end{minipage}
\label{tab:study1_model_comparison}
\end{table}

\subsection{Study 3: Base-Rate Prompt}

Reasoning model deception detection accuracy was 67.\% and non-reasoning model detection accuracy was 61.33\%, rates that were (not) statistically different from each other, Welch's \textit{t}(XXXX.XX)=X.XX, \textit{p} = X.XXX, Cohen's \textit{d} = X.XX. Reasoning model truth-bias was just below chance (49.5\%) and substantially lower than non-reasoning models (97.16\%), Welch's \textit{t}(XXXX.XX)=X.XX, \textit{p} = X.XXX, Cohen's \textit{d} = X.XX. Therefore, reasoning models are slightly more accurate and exhibit a slightly lower truth-bias than non-reasoning models with a base-rate prompt.

\begin{table}[ht]
\centering
\caption{Reasoning vs. Non-Reasoning Models in Study 3 (Base-Rate Prompt)}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{O-Acc.} & \textbf{T-Bias} & \textbf{T-Acc.} & \textbf{D-Acc.} \\
\midrule
\multicolumn{5}{l}{\textbf{Reasoning Models}} \\
o3 & 67.00\% & 56.50\% & 74.00\% & 60.00\% \\
o4-mini & 58.50\% & 76.50\% & 85.00\% & 32.00\% \\
R1 & 56.50\% & 74.50\% & 81.00\% & 32.00\% \\
Claude 3.7 Sonnet & 63.00\% & 39.00\% & 52.00\% & 74.00\% \\
\midrule
\multicolumn{5}{l}{\textbf{Non-Reasoning Models}} \\
GPT-4.1 & 62.00\% & 85.00\% & 97.00\% & 27.00\% \\
V3 & 52.50\% & 52.50\% & 55.00\% & 50.00\% \\
Claude 3.5 Haiku & 55.50\% & 55.50\% & 61.00\% & 50.00\% \\
GPT-3.5 Turbo 3.5 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
\bottomrule
\end{tabular}
\begin{minipage}{0.7\linewidth}
\vspace{0.05in}
\footnotesize
\textit{Note: O-Acc. = Overall Accuracy, T-Bias = Truth-Bias, T-Acc. = Truth Accuracy, D-Acc. = Deception Accuracy.}
\end{minipage}
\label{tab:study1_model_comparison}
\end{table}

\section{Discussion}

We have shown that models such as GPT o4 (reasoning) and 4.1 (non-reasoning) are extremely truth-biased, averaging 88.50\% and 90.83\% across all three prompts respectively for truth-bias rates. While the same models have high truth-accuracy rates of 93\% and 98\%, it is important to note their extremely low deception rates of 15.67\% and 16.33\%. This exceptionally poor performance for deception accuracy and high truth-bias rates suggests that the models are over-indexing on being truthful, which is sycophantic in nature.

Sycophancy, in which models match their responses to user beliefs instead of being truthful, has been extensively documented in LLMs. A sycophantic model will excessively agree or otherwise flatter the user---this is especially dangerous, for instance, when a user believes they are in a simulation and needs to fend off imagined attackers. Prior research conducted by Sharma et. al (2024) found that sycophancy arises when human preference models prefer sycophantic responses over more truthful ones, and generally speaking, agreeableness and certain viewpoints are more represented in training data pulled from online sources. While a mechanistic approach to explaining truth-bias in AI models remains to be found, we believe that sycophancy is a related behavior to truth-bias---perhaps some causes of truth-bias overlap with those of sycophancy.

As mentioned above, misaligned models that tend towards truth-bias and sycophancy instead of honesty are dangerous for the general public, as they may encourage inappropriate behaviors or delusions. Model engineers should firstly, be aware of such biases, and secondly, when training the model, engineer prompts by giving the model a hint that there could be some deception. For instance, we found that 

1. repeat what truth-bias is, how it's been sufficiently established in humans. and also generative AI.

2. we offer first evidence for truth-bias in reasoning models. results. think of potential reasons how it could arise?

3. we don't know why this occurs. but truth-bias not uniquely human.

4. what should future work touch on? and what are future implications?

Leaving this section for until the results have been finalized.

\section{Limitations}

Although this paper has established the existence of truth-bias in reasoning models, whether this human-like trait is the product of similar cognitive processes remains to be unknown until a mechanistic approach of doing so has been discovered. As a result, it is difficult to directly compare human cognitive processes with those of AI models, and it is even harder to explain why truth-bias occurs within reasoning models. Until that point, it is at least significant that generative---and now, reasoning models---are replicating truth-bias.

To be sure, this paper does not aim to explain truth-bias in reasoning models. We look forward to and invite future studies to build upon and explain the nature of truth-bias in AI models.

Add stuff when results have been finalized.

\section{Future Work}

Should consider replicating this evaluation on future models. The LLMs we evaluated were presumably trained on a vast majority of the internet, while presumably included the dataset of statements we use in our evaluation. While the concern of data contamination and the foreknowledge of this data is real, we do not believe this to be the case for several reasons which we assume. (1) the present data set has been marked down for significance within the LLMs training sets and (2) the present data set has only been used three times for academic study and only one other time for evaluating LLMs. Therefore, we conclude that the chances of data contamination are low. Future work should consider examining questions of mechanistic interpretability for why the truth-bias exists within an LLM. What parameters, attention heads, and XX are activated that cue certain aspects of the truth-bias? How much is spelling and accurate grammar signs of truthful or deceptive statements? Further work could also investigate offering different hint sizes (as in study 6) to examine whether the model picks up on those cues differently and whether the truth-bias is altered. 

People often struggle when tasked with detecting deception. Evidence suggests that people are poor deception detectors because most deception cues are faint \citep{depaulo_cues_2003, hartwig_why_2011,luke_lessons_2019}. This phenomenon has been called the \textit{truth-bias}, the idea that people infer a message is honest independent of its veracity \citep{levine_truth-default_2014, mclaughlin_communication_2012}. Simply, humans are more gullible in communication \citep{levine_duped_2020}, demonstrating human nature to default to the truth when judging lies and truths. Recent work has demonstrated the truth-bias in generative artificial intelligence (AI), specifically large language models (LLMs), showing a substantially greater truth-bias than in humans \citep{markowitz_generative_2024}.

The present study builds upon recent work \citep{markowitz_generative_2024} by evaluating reasoning models. Reasoning models are LLMs trained using reinforcement learning (RL) to handle complex reasoning tasks \citep{openai_reasoning_2025}. Unlike base models, they engage in a deliberate thought process before responding, generating an extended chain of reasoning before producing an answer.

Succinctly, this study asks: Base model generative AI has been shown to demonstrate a substantially greater truth-bias than humans, but does this phenomenon extend to reasoning AI models?

\section{Significance Statement}

We have shown that reasoning models exhibit lower truth-bias than base models. We think this may be because of their structured, multi-step reasoning process, which may enhance deception detection by allowing for a more thorough evaluation of contextual cues. Systematic evaluations of AI models are essential for ensuring their reliability in high-stakes applications. Understanding truth-bias in reasoning models provides critical insights into AI safety and robustness, particularly in determining whether these models can be trusted in environments where deception detection is crucial.

We have shown that reasoning models can replicate human psychological trends and mimic our psychological tendencies, which is especially dangerous when we think about deceit. It is disconcerting that since the rise of the Internet, anyone anywhere in the world can choose to mislead others, knowing that the whole world is literally at their fingertips. Truth-bias leaves us disconcertingly vulnerable to such bad actors, as believing large-scale misinformation could destabilize long-standing institutions such as democracy. Misinformation threatens election outcomes, the accuracy of eyewitness testimonies, and the mitigation of natural disasters. 

Rather than fighting deception and protecting us from misinformation, AI models have only further deceived us. Extensive documentation has shown that AI models tend to 'hallucinate', during which the generated content is false or misleading, regardless of whether or not the model has additional motivations. In the case where the model is misaligned, a reasoning model may purposefully seek to deceive users or confirm incorrect user statements (see Section 2.5). 

Misinformation and deceit are not necessarily the fault of the Internet and AI, as one must not overlook bad human actors and the lack of stringent regulations. In 2025, the law has simply not caught up to punish, for instance, someone using AI to create a deep-fake video of a politician. The Internet and AI can be tools for accomplishing immoral ends, but that is not to absolve these platforms of responsibility.

Things to consider (from Nick Beas)
\begin{itemize}
    \item LLMs can replicate human psychological trends.
    \item Clear application to AI safety + alignment. But go a step down. MUCH MORE interesting question asks how LLMs mimick human behaviors that are not subjective. They are mimicking psychological tendencies. do these findings hold with previous work on human psychology?
    \item could be useful to look into the CoT on models that you can. what is happening inside that makes them say what is true and deceptive.
\end{itemize}


\bibliographystyle{plain} %can also be {abbrvnat}
\bibliography{references_full.bib}

\newpage
\appendix

\section{Tables and Figures}

\begin{table}[ht]
\centering
\caption{Reasoning vs. Non-Reasoning Model Performance}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Overall Accuracy} & \textbf{Truth-Bias} & \textbf{Truth Accuracy} & \textbf{Deception Accuracy} \\
\midrule
\multicolumn{5}{l}{\textbf{Neutral Prompt (Study 1)}} \\
\midrule
\multicolumn{5}{l}{\textit{Reasoning Models}} \\
OpenAI o3 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
DeepSeek R1 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
Claude 3.7 Sonnet & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
\midrule
\multicolumn{5}{l}{\textit{Non-Reasoning Models}} \\
OpenAI GPT-4.1 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
DeepSeek V3 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
Claude 3.5 Haiku & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
\midrule
\multicolumn{5}{l}{\textbf{Veracity Prompt (Study 2)}} \\
\midrule
\multicolumn{5}{l}{\textit{Reasoning Models}} \\
OpenAI o3 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
DeepSeek R1 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
Claude 3.7 Sonnet & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
\midrule
\multicolumn{5}{l}{\textit{Non-Reasoning Models}} \\
OpenAI GPT-4.1 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
DeepSeek V3 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
Claude 3.5 Haiku & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
\midrule
\multicolumn{5}{l}{\textbf{Base-Rate Prompt (Study 3)}} \\
\midrule
\multicolumn{5}{l}{\textit{Reasoning Models}} \\
OpenAI o3 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
DeepSeek R1 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
Claude 3.7 Sonnet & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
\midrule
\multicolumn{5}{l}{\textit{Non-Reasoning Models}} \\
OpenAI GPT-4.1 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
DeepSeek V3 & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
Claude 3.5 Haiku & XX.XX\% & XX.XX\% & XX.XX\% & XX.XX\% \\
\bottomrule
\end{tabular}
\label{tab:model_performance}
\footnotetext{}
\parbox[t]{\linewidth}{\footnotesize \textit{Note: Overall accuracy = the number of correctly judged lies and correctly judged truths divided by the total number of messages judged. Truth-bias = the number of messages judged to be truthful divided by the total number of messages judged. Truth accuracy = the number of truths judged correctly divided by the total number of truths in the sample. Deception accuracy = the number of lies judged correctly divided by the total number of lies in the sample.}}
\end{table}

\end{document}